# 선형대수학 Linear Algebra

## 벡터와 스칼라 Vector and Scalar
* 수학에서는 데이터 여러 개를 한 줄에 담아낼 수 있게 만든 것을 벡터라고 한다. 
    + 프로그래밍에서도 여러 개의 데이터를 하나의 열에 담아 둔 것을 벡터 또는 배열이라고 한다.
* 벡터의 표기 : **a, b** 
* 벡터의 덧셈은 서로 대응하는 성분끼리 덧셈을 한다.

### 단어의 벡터화 word2vec
* *단어들을 마치 벡터처럼 취급*하여 일반적인 벡터 연산처럼 단어들 간의 연산이 가능해진다. 
* [**word2vec**](https://arxiv.org/pdf/1301.3781.pdf)은 단어를 벡터처럼 다르는 방법으로 2013년 구글이 발표한 기법이다. 
* 또한 2016년 페이스북에서 fastText라고 하는 *언어를 벡터로 표현하는 기법* 또한 제안되었다. 

### 유향선분 Directed Segment
* 벡터는 **방향**과 **거리**의 두가지 요소를 지니고 있다. 이러한 방향과 거리를 나타내는 화살표를 **유향선분**이라고 한다.
* 벡터의 연산의 그래프

![vec](https://user-images.githubusercontent.com/28593767/111561402-b89dcd80-87d7-11eb-8b43-6afdba1c5427.png)


## 내적 Inner product

![dot1](https://user-images.githubusercontent.com/28593767/111561405-b9cefa80-87d7-11eb-9a12-f61756107c0a.png)

![dot2](https://user-images.githubusercontent.com/28593767/111561409-ba679100-87d7-11eb-88dc-ac0a67012776.png)


* 내적의 계산은 벡터끼리 서로 대응하는 성분끼리 곱한 다음 그것을 모두 더한 값을 의미하고 <**a, b**> 라고 표기한다.
* 다른 말로 점곱(Dot product)라고도 한다.
* **벡터와 벡터를 내적한 결과는 벡터가 아니라 스칼라**가 되고 **서로 다른 차원의 벡터끼리는 계산을 할 수 없다**.

<img width="400" alt="orthogonal" src="https://user-images.githubusercontent.com/28593767/111566301-41b90280-87e0-11eb-9e3f-9aa78f13642d.png">

* 두 개의 벡터가 **a, b**가 서로 **직교한다**는 것은 두 벡터가 이루는 각이 90°라는 의미이다.
* 두 벡터가 직교할 때 내적 <**a, b**> = 0 이다.
* 다시 말해, 두 벡터가 직교하는지 확인하려면 **두 벡터의 내적이 0인지 확인**하면 된다.


## 노름 Norm
* 벡터의 이동 거리를 노름이라고 한다.
* 맨하튼 거리 Manhattan distance(L1 노름) : 가로와 세로의 바둑판 모양을 따라 움직이는 방법이다.

    ![norm1](https://user-images.githubusercontent.com/28593767/111561396-b76ca080-87d7-11eb-8c17-f4249853690d.png)

    + 벡터 각 성분들의 절댓값을 구한 다음, 모두 더하면 된다.
* 유클리드 거리 Euclidean distance(L2 노름) : 시작점부터 목적지까지 직선으로 움직이는 방법이다.

    ![norm2](https://user-images.githubusercontent.com/28593767/111561478-db2fe680-87d7-11eb-93ef-5e62fd067ad4.png)

    + 피타고라스 정리를 이용해서 구하면 된다.

### 인공지능에서의 활용
* L1 노름과 L2 노름은 선형회귀 모델의 정규화 항에서 사용할 수 있다.
* **overfitting** 현상이 발생할 시, 선형회귀 모델에 정규화 항을 덧붙여 계수의 절댓값이나 제곱한 값이 너무 커지지 않도록 만들어 줘야 한다. 
* 즉, **정규화 항은 계수가 너무 커지지 않게 하기 위한 일정의 패널티나 핸디캡같은 억제 기능**을 한다.


## 코사인 유사도 Cosine Similarity

![cos](https://user-images.githubusercontent.com/28593767/111565970-ac1d7300-87df-11eb-92bc-7b16346392da.png)

* 코사인 유사도는 내적의 정의식과 L2 노름의 정의식을 활용하여 구할 수 있고 cos(**a, b**)로 표현한다.
* 코사인 유사도의 값은 −1≤ cos(**a, b**) ≤ 1 의 구간 안에 있다.
* 유사도가 -1 일 때는 두 벡터가 서로 *반대 방향으로 평행*이고, 0 일 때는 *직교 상태*, 그리고 1일 때는 서로 *같은 방향으로 평행*이 된다.
* 다시 말해, **코사인 유사도가 더 높다는 말은 벡터가 더 닮았다**는 의미라고 할 수 있다.

### 인공지능에서의 활용
* AI가 텍스트를 분석할 때는 단어나 문장을 벡터로 처리하는데 이 때 벡터로 만들어진 단어나 문장들은 서로의 관계성을 파악할 때 코사인 유사도를 사용한다.
* 즉, 코사인 유사도가 높을수록 해당 단어나 문장들은 더 가까운 관계라고 할 수 있다.


## 행렬 Matrix
* 행렬은 복잡한 계산을 간단한 형태로 표현할 때 사용한다.
* 행렬은 같은 차원의 벡터를 여러 개 쌓아놓은 것과 같다.
* 행렬을 더하거나 뺄 때는 서로 대응하는 성분끼리 덧셈, 뺄셈을 한다.
* **행렬의 곱셈은 곧 벡터의 내적을 확장한 것**이라 할 수 있다.
    + ![matrix](https://user-images.githubusercontent.com/28593767/111565977-ad4ea000-87df-11eb-8d92-dbc5410c055d.png)
    + (m * n) 행렬과 (n * l) 행렬을 곱하면 그 결과는 (m * l) 행렬이 된다.
    + 행렬의 곱셈에서는 *AB = BA* 같은 **교환법칙이 성립하지 않는다**.

