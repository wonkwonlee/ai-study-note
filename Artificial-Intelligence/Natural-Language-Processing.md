# 자연어 처리 Natural Language Processing

## 자연어 처리 NLP
![nlp](https://user-images.githubusercontent.com/28593767/115187514-6b868180-a11e-11eb-8533-d0914bcb1914.png)

* 인간이 일상에서 사용하는 언어를 자연어라고 한다.
* 컴퓨터 분야에서는 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 **자연어 처리(Natural Language Processing)** 혹은 줄여서 **NLP**라고 한다.
* 일반적으로 문장을 일정한 의미가 있는 **토큰(Token)** 이라는 가장 작은 단어들로 나눈 뒤 나눠진 단어들을 이용해 의미를 분석한다.
* 토큰의 단위는 **토크나이징(Tokenizing)** 방법에 따라 달라질 수 있지만 일반적으로 *일정한 의미가 있는 가장 작은 정보 단위로 결정*된다.
* *토크나이징은 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업이며 주로 텍스트 전처리 과정에서 사용*된다.



### KoNLPy
* [**KoNLPy**](https://konlpy-ko.readthedocs.io/ko/v0.4.3/) 는 한국어 토크나이징을 지원하는 파이썬 라이브러리로 한국어 자연어 처리에 많이 사용된다.
* 한국어 문장을 분석하려면 토크나이징 작업을 제일 먼저 수행해야 하는데, 이때 토큰 단위를 어떻게 정의하느냐에 따라 자연어 처리 성능에 영향을 미친다.
    + 일정한 의미가 있는 가장 작은 말의 단위, 즉 의미가 더 이상 쪼개지지 않는 **형태소(Morpheme)** 를 토큰 단위로 사용한다.
* 영어의 경우 단어의 변화가 크지 않고, 띄어쓰기로 단어를 구분하기 때문에 공백을 기준으로 토크나이징을 수행해도 큰 문제 없지만 *한국어는 명사와 조사를 띄어 쓰지 않고, 용언에 따라 여러 가지 어미가 붙기 때문에 띄어쓰기만으로는 토크나이징할 수 없다*.
* 따라서 형태소 분석기를 이용하여 문장에서 형태소를 추출하면서 형태소의 뜻과 문맥을 고려해 품사 태깅을 해줘야 한다.

### Kkma
* [**Kkma**](http://kkma.snu.ac.kr/documents/?doc=postag) 는 서울대학교 IDS(Intelligent Data Systems) 연구실에서 자연어 처리를 위해 개발한 한국형 형태소 분석기로 *꼬꼬마*라고 발음한다.
* Kkma는 다음 4가지 함수를 제공한다.
<img width="1292" alt="kkma" src="https://user-images.githubusercontent.com/28593767/115185952-cf5b7b00-a11b-11eb-8900-6b10475a3618.png">

### Komoran 
* [**Komoran(Korean Morphological ANalyzer)**](https://www.shineware.co.kr/products/komoran/#demo?utm_source=komoran-kr&utm_medium=Referral&utm_campaign=github-demo) 은 Shinware에서 개발한 자바 기반 한국어 형태소 분석기로 *코모란*이라고 발음한다.
* Komoran은 다음 3가지 함수를 제공한다.
![komoran](https://user-images.githubusercontent.com/28593767/115186644-0c743d00-a11d-11eb-9d0e-9e10321a5402.png)

### Okt
* [**Okt(Opensource Korean Text Processor)**](https://openkoreantext.org/) 는 트위터에서 개발한 Twitter 한국어 처리기에서 파생된 오픈소스 한국어 처리기이다.
* Okt의 경우 앞서 소개한 형태소 분석기들보다 분석되는 품사 정보는 작지만 분석 속도는 제일 빠르고 또한 normalize() 함수를 지원해 오타가 섞인 문장을 정규화해서 처리하는데 효과적이지만 성능이 뛰어나지는 않다.
* Okt는 다음 5가지 함수를 제공한다.
![okt](https://user-images.githubusercontent.com/28593767/115187507-69bcbe00-a11e-11eb-9740-bb14fa6aa3ec.png)

### 한국어 토크나이징
* 영어의 경우 단순히 토큰 정보만 필요하다면 띄어쓰기만 하더라도 훌륭한 결과를 보여준다. 
* 하지만 *한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러 가지 어미가 붙기 때문에 띄어쓰기만으로는 토크나이징을 할 수 없다*. 
* 따라서 KoNLPy의 형태소 분석기를 이용해 형태소 단위의 토큰과 품사 정보까지 추출하고 추출된 정보에서 필요 없는 정보를 제거하는 **전처리(Preprocessing)** 과정이 추가되어야 한다.

![konlpy](https://user-images.githubusercontent.com/28593767/115188134-71c92d80-a11f-11eb-9dbc-e3db0889475e.png)


## 임베딩 Embedding
* 컴퓨터는 수치 연산만 가능하기 때문에 자연어를 숫자나 벡터 형태로 변환해야 한다. 
* 임베딩은 **단어나 문장을 수치화해 벡터 공간으로 표현하는 과정**을 의미한다. 
* 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다. 
* 임베딩 기법에는 문장 전체를 벡터로 표현하는 문장 임베딩과 개별 단어를 벡터로 표현하는 단어 임베딩이 있다.
    + 문장 임베딩의 경우 *전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점*이 있다.
    + 하지만 *임베딩하기 위해 많은 문장 데이터가 필요하며 학습하는데 비용*이 많이 들어간다.
    + 반면 단어 임베딩은 문장 임베딩에 비해 학습 방법이 간단해 *실무에서 많이 사용*된다.
    + 하지만 단어 임베딩은 *동음이의어에 대한 구분을 하지 않기 때문에 의미가 다르더라도 단어의 형태가 같다면 동일한 벡터값으로 표현되는 단점*이 있다.
* 단어 임베딩은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법으로 의미와 문법적 정보를 지니고 있으며, 단어를 표현하는 방법에 따라 다양한 모델이 존재한다.

### 원-핫 인코딩 One-Hot Encoding
<img width="484" alt="onehot" src="https://user-images.githubusercontent.com/28593767/115192638-09317f00-a126-11eb-8c21-4569f91c4aa4.png">

* 원-핫 인코딩은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법으로 *단 하나의 값만 1이고 나머지 요솟값은 0인 인코딩*을 의미한다. 
* 원-핫 인코딩으로 나온 결과를 **원-핫 벡터(One-Hot Vector)** 라 하며, 전체 요소 중 단 하나의 값만 1이기 때문에 **희소 벡터(Sparse Vector)** 라고 한다.
* 원-핫 인코딩을 하기 위해서는 단어 집합이라 불리는 사전을 먼저 만들어야 한다. 
    + 사전이란 말뭉치에서 나오는 서로 다른 모든 단어의 집합을 의미한다. 
    + 말뭉치에 존재하는 모든 단어의 수가 원-핫 벡터의 차원을 결정한다.
    + 사전 내 단어 순서대로 고유한 인덱스 번호를 부여해 단어의 인덱스 번호가 원-핫 인코딩에서 1의 값을 가지는 요소의 위치가 부여된다.
* 이처럼 단어가 희소 벡터로 표현되는 방식을 **희소 표현(Sparse Representation)** 이라고 한다.
    + 희소 표현은 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적인 장점이 있지만 *단어 사전의 크기가 커질수록 메모리 낭비와 계산 복잡도가 커지는 단점*이 있다.
    + 또한 *기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계를 단어 임베딩에 표현할 수 없다는 단점*도 있다.

### 분산 표현 Distributed Representation
<img width="394" alt="distri" src="https://user-images.githubusercontent.com/28593767/115192633-0767bb80-a126-11eb-9314-086c7f80e740.png">

* 분산 표현은 한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현되어 붙여진 이름이다.
    + 원하는 차원에 데이터를 최대한 밀집시킬 수 있어 **밀집 표현(Dense Representation)** 이라 부르기도 하며 밀집 표현으로 만들어 진 벡터를 **밀집 벡터(Dense Vector)** 라고 한다.
* 희소 표현과 달리 *각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법*으로 하나의 차원에 다양한 정보를 가지고 있다.
* 분산 표현의 첫번째 장점은 **임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다**는 점이다.
    + 희소 표현 방식은 단어를 표현하는 데 너무 많은 차원이 필요하고 단어 사전이 커질수록 비효율적이고 희소 벡터이기 때문에 대부분의 값이 0이 된다.
    + 입력 데이터의 차원이 너무 높아지면 신경망 모델의 학습이 어려워지는 **차원의 저주(Curse of Dimentionality)** 문제가 발생한다.
* 또한 분산 표현의 두 번째 장점은 **임베딩 벡터에는 단어의 의미, 주변 단어간의 관계 등 많은 정보가 표현되어 있어 일반화 능력이 뛰어나다**는 점이다.
    + 예를 들어 남자와 남성이라는 단어가 있을 때 희소 표현 방식에는 그저 단 하나의 요솟값에 불과하다.
    + 즉, 남자와 남성의 관계가 전혀 표현되어 있지 않는다.
    + 하지만 분산 표현에서는 *벡터 공간 상에서 유사한 의미를 갖는 단어들은 비슷한 위치에 분포*되어 있기 때문에 남자와 남성의 단어 위치는 매우 가깝다.

### Word2Vec 모델
* Word2Vec 모델은 신경망 기반 단어 임베딩의 대표적인 방법으로 2013년에 구글에서 발표했으며 가장 많이 사용되는 단어 임베딩 모델이다.
* 기존 신경망 기반의 단어 임베딩 모델에 비해 구조상 차이는 크게 없지만 *계산량을 획기적으로 줄여 빠른 학습을 가능*하게 하였다.
* Word2Vec 모델은 **CBOW(Continuous Bag of Words)** 와 **skip-gram** 두 가지 모델로 제안되었다.
    
<img width="891" alt="word2vec" src="https://user-images.githubusercontent.com/28593767/115327256-8f52d180-a1c9-11eb-9bb9-1497384c4c0f.png">

* CBOW 모델은 **맥락(Context Word)이라 표현되는 주변 단어들을 이용해 타겟 단어를 예측**하는 신경망 모델이다. 
    + 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용한다.
    + CBOW 모델은 *타깃 단어의 손실만 계산하면 되기 때문에 학습 속도가 빠른 장점*이 있다.
* skip-gram 모델은 CBOW 모델과 반대로 **하나의 타깃 단어를 이용해 주변 단어들을 예측**하는 신경망 모델이다.
    + skip-gram 모델은 입출력이 CBOW 모델과 반대로 되어 있기 때문에 예측해야 하는 맥락이 많다.
    + 따라서 *단어 분산 표현력이 우수해 CBOW 모델에 비해 임베딩 품질이 우수*하다.
* CBOW 모델에서는 타깃 단어를 예측하기 위해 앞뒤 단어를 확인하는데 이 때 *앞뒤로 몇 개의 단어까지 확인할지 결정하는 범위*를 **윈도우(Window)** 라고 한다. 

<img width="915" alt="window_size" src="https://user-images.githubusercontent.com/28593767/115327554-1f911680-a1ca-11eb-8c6b-37b7d5dbe4fd.png">

* Word2Vec의 단어 임베딩은 해당 단어를 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치시킨다.
* 벡터 특성상 의미에 따라 방향성을 지니고 임베딩된 벡터들 간 연산이 가능하기 때문에 단어간 관계를 계산할 수 있다.
    + 왕과 여왕의 방향 차이만큼 남자와 여자의 방향 차이가 생긴다.

<img width="398" alt="wordvec" src="https://user-images.githubusercontent.com/28593767/115328362-7a773d80-a1cb-11eb-9c44-0864b2dacba5.png">


## Text Similarity Algorithm
* 인간은 두 개의 문장에 동일한 단어나 의미상 비슷한 단어들이 얼마나 분포되어 있는지 직감적으로 파악한다.
* 컴퓨터는 *임베딩으로 각 단어들의 벡터를 구한 다음 벡터 간의 거리를 계산하는 방법으로 단어 간의 의미가 얼마나 유사 한지 계산*할 수 있다.
* Q&A 챗봇 개발을 위해서는 챗봇 엔진에 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사 한지 계산할 수 있어야 적절한 답변을 출력할 수 있다.
* 두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화해야 하는데 이 때 언어 모델에 따라 통계를 이용하는 방법과 인공 신경망을 이용하는 방법으로 나눌 수 있다.
    + Word2Vec은 인공 신경망을 이용한 방법이고 이제 통계적인 방법을 이용해 유사도를 계산하는 방법을 살펴본다.

### n-gram
![n-gram](https://user-images.githubusercontent.com/28593767/115339998-e021f480-a1e0-11eb-8ca5-a8b3303bdaca.png)

* n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미한다. 
* n-gram은 문장에서 n개의 단어를 토큰으로 사용하여 *이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법*이다.
* *서로 다른 문장을 n-gram으로 비교하면 단어의 출현 빈도에 기반한 유사도를 계산*할 수 있고 이를 통해 논문 인용이나 도용 정도를 조사할 수 있다.
    + 구현하기는 쉽지만 학습 말뭉치 품질만 좋다면 괜찮은 성능을 보여준다. 

![similarity](https://user-images.githubusercontent.com/28593767/115339999-e1532180-a1e0-11eb-97cb-0a0c1e3e09bf.png)

* n-gram을 이용한 문장 간의 유사도를 계산
    1. 문장을 n-gram으로 토큰을 분리하여 **단어 문서 행렬(Term-Document Matrix, TDM)** 을 만든다.
    2. 두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구한다.
* **tf(Term Frequency)** 는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하며 tokens는 해당 문장에서 전체 토큰 수를 의미한다.
    + 여기서 토큰은 n-gram으로 분리된 단어를 뜻한다.
    + 즉, 위의 Similarity 수식은 *기준이 되는 문장 A의 전체 토큰 중 A와 B에 동일한 토큰이 얼마나 있는지를 비율로 나타낸 수식*이다.
* 실무에서는 문서 단위로 유사도를 구하는 경우가 많고 해당 문서에서 단어들이 얼마나 나오는지 출현 빈도를 행렬로 표현한다.



