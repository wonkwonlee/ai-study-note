{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-cnn-practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOx6peLFTkXUpgpEORHOAB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wonkwonlee/likelion-k-digital-training-AI/blob/main/Artificial-Intelligence/nlp_cnn_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0U-bRFsSdUX",
        "outputId": "6451554a-d8ce-401f-cdbb-45017259efa4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh-pKVdqSNhz"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "# csv 챗봇 데이터 로드\n",
        "train_file = \"/content/drive/MyDrive/Colab Notebooks/data_nlp/chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# features의 각 문장 별로 단어 시퀀스를 생성하여 코퍼스에 저장\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features] \n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "# 단어를 시퀀스 번호로 변환\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "# 단어 시퀀스 벡터 크기\n",
        "MAX_SEQ_LEN = 15 \n",
        "# 빈 공간을 0으로 채우는 패딩 작업\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tspYxFSS4Ld",
        "outputId": "7473cf5a-e847-4146-9655-ac7ae5d8c131"
      },
      "source": [
        "# 학습용, 검증용, 테스트용 데이터셋 생성 (7:2:1)\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels)) \n",
        "ds = ds.shuffle(len(features))  # 데이터 셔플\n",
        "\n",
        "train_size = int(len(padded_seqs) * 0.7)  # 학습용 데이터\n",
        "val_size = int(len(padded_seqs) * 0.2)    # 검증용 데이터\n",
        "test_size = int(len(padded_seqs) * 0.1)   # 테스트용 데이터\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
        "\n",
        "# 하이퍼파라미터 기본값 설정 (상황에 따라 변화 가능) \n",
        "dropout_prob = 0.5  # 드롭아웃 확률\n",
        "EMB_SIZE = 128  # 임베딩 사이즈\n",
        "EPOCH = 5 # 에폭값\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수\n",
        "\n",
        "# CNN 모델 정의 \n",
        "# 입력 레이어 (단어 시퀀스 벡터 크기)\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "# 임베딩 레이어\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer) \n",
        "# 드롭아웃 레이어 (기본 드롭아웃 확률은 0.5) - 과적합 대비\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "# 컨볼루션 레이어 1\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1) # 풀링 레이어 1\n",
        "# 컨볼루션 레이어 2\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb) \n",
        "pool2 = GlobalMaxPool1D()(conv2) # 풀링 레이어 2\n",
        "# 컨볼루션 레이어 3\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb) \n",
        "pool3 = GlobalMaxPool1D()(conv3) # 풀링 레이어 3\n",
        "\n",
        "# 병렬 처리된 feature map 결합\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "# 완전 연결 레이어 (relu 활성화 함수)\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat) \n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "# 출력 노드: 3가지 감정 클래스 \n",
        "logits = Dense(3, name='logits')(dropout_hidden) \n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델 생성 \n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "# ADAM 옵티마이저 사용\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 41s 19ms/step - loss: 0.9847 - accuracy: 0.4894 - val_loss: 0.5498 - val_accuracy: 0.8105\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.5243 - accuracy: 0.7985 - val_loss: 0.2751 - val_accuracy: 0.9167\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.3130 - accuracy: 0.8996 - val_loss: 0.1616 - val_accuracy: 0.9526\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.2002 - accuracy: 0.9356 - val_loss: 0.1039 - val_accuracy: 0.9687\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 8s 19ms/step - loss: 0.1281 - accuracy: 0.9610 - val_loss: 0.0653 - val_accuracy: 0.9827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3090489490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c40CRgzaUbI8",
        "outputId": "0d57e520-428b-4b57-f665-31e8825a6479"
      },
      "source": [
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1) \n",
        "print('Accuracy: %f' % (accuracy * 100)) \n",
        "print('loss: %f' % (loss))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9839\n",
            "Accuracy: 98.392552\n",
            "loss: 0.056591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zwMeTFwUdr-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}