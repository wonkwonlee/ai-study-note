# ADAM 알고리즘 Adaptive Moments Estimation

## 딥러닝에서의 다차원 분류 및 ADAM 알고리즘
<img width="500" alt="office31" src="https://user-images.githubusercontent.com/28593767/114334392-745ade80-9b85-11eb-81da-011bd93430cf.png">

* *두 가지 차원에서 동시에 분류를 수행하는 다차원 분류 신경망*에 ADAM 알고리즘을 적용하여 신경망 모델의 학습 품질을 향상시킨다.
* office31 데이터셋은 컴퓨터 비전 분야에서의 전이학습 연구용으로 구축된 표준 벤치마크 데이터셋으로 수집 방법에 따른 3가지 도메인과 31가지 사무용품 이미지 4,652장으로 구성되어 있다.
* 즉, 한 장의 사진은 31가지 사무용품 중 어떠한 품목에 속하는지, 그리고 어떠한 도메인에서 수집이 되었는지 이중으로 레이블링 되어 있다.

### 전이 학습 Transfer Learning
* 전이학습은 *한 도메인에서 학습시킨 결과를 다른 도메인에서 활용하여 학습 효과를 높이는 학습 기법*을 말한다.
* 예를 들어 amazon 도메인에 속한 데이터들에 품목 레이블 정보가 모두 태깅되어 있어서 이를 이용해 학습을 시켜 높은 성능을 발휘하는 모델이 있다고 가정할 때, 이미 확보된 amazon 도메인 모델의 학습 정보를 잘 활용하면 품목 레이블 정보가 매우 적거나 없는 webcam 도메인에서 그나마 나은 성능을 확보할 수 있다.
* 즉, **학습에 필요한 데이터의 부족 현상을 해결하고자 연구 되어지는 분야**가 바로 전이 학습이다.


### 다차원 분류 Multi-Dimentional Classification
* 이미지를 넣었을 때 도메인과 품목을 동시에 판별해 주는 딥러닝 모델을 만든다고 한다면 도메인과 품목, 즉 *두 가지 차원에서의 분류를 한 번에 수행하고 결과 또한 동시에 보여줘야 하기에 난이도가 높다.*
* **차원 축소(Dimentionality Reduction)** 이란 *복합 출력에 대한 학습 방식으로 2차원 분류를 1차원으로 줄이는 방법*이다.
* 딥러닝에서는 같은 퍼셉트론이라도 어떠한 학습 과정을 거치냐에 따라 역할이 달라지게 된다. 
    + 달라지는 것은 가중치나 편향 같은 파라미터 값의 구성이며, 똑같은 퍼셉트론이라도 학습시키기에 따라 전복의 고리 수를 추정할 수도 있고 꽃 이미지를 구분할 수도 있다.
* 즉, **동일한 구조의 신경망을 다양한 용도에 이용하기 위해서는 후처리 과정의 처리 방법만 변경** 하면 된다.
* 이러한 후처리 과정은 **후처리 순전파** 과정과 **후처리 역전파** 과정으로 구성되어 있다.
    + 후처리 순전파 과정은 *신경망 순전파 처리 과정에서 얻어진 출력으로 부터 손실 함수를 계산하는 과정*을 의미한다.
    + 후처리 역전파 과정은 *출력에 따른 각 성분들의 손실 기울기를 계산해서 신경망 역전파 처리 과정에 도움을 주는 과정*을 의미한다.

<img width="567" alt="IO" src="https://user-images.githubusercontent.com/28593767/114334399-77ee6580-9b85-11eb-9571-ada347e4b2a2.png">

* office31 데이터셋을 이용한 다차원 분류의 경우 신경망은 *이미지 픽셀 수 만큼의 입력 벡터 크기*를 가지며 *출력 벡터 크기는 도메인 판별에 사용될 성분 3가지와 품목 판별에 사용될 성분 31 가지를 더한 34가지 성분*을 가진다.
* 만약 필요한 출력별로 별도의 신경망을 구성한다면 어떻게 될까?
    + 출력 계층을 구성하는 퍼셉트론은 맡은 임무에 따라 역할이 확실하게 다르기 때문에 출력 계층만 따지면 신경망을 분리하든 말든 별 차이가 없다.
    + 하지만 *은닉 계층에 속한 퍼셉트론은 모든 출력 계층 퍼셉트론에 영향*을 주고 *모든 출력 계층 퍼셉트론으로 부터 역전파 피드백*을 받으므로 복합 출력으로 처리하는 경우와 신경망을 따로 구성하는 경우에 커다란 차이가 발생한다.
    + 따라서 각각의 출력을 위한 별도의 은닉 계층을 힘들게 따로 학습시키는 것보다 **모든 출력에 연결된 하나의 은닉 계층이 이러한 공통 특성을 포착하도록 학습시키면 계산량을 절감하면서도 성능을 향상 시킬 수 있다.**
* 결론적으로 출력에 따라 별도의 신경망을 구성하는 것보다는 *하나의 신경망 출력을 복합 출력으로 처리하는 방법이 더 바람직하다.*

### 복합 출력의 학습 방법
<img width="622" alt="MDC" src="https://user-images.githubusercontent.com/28593767/114334402-7886fc00-9b85-11eb-92bb-1aa76076c17f.png">

* 하나의 신경망이 두 성분으로 구성되는 복합 출력을 낼 때의 학습 방법은 비교적 간단하다.
* 레이블 정보로 준비된 정답 *y1, y2* 와 복합 출력의 두 성분 *output1, output2* 를 비교해 손실값 *L1, L2* 를 얻을 때, 전체적인 손실 함수값을 *L = L1 + L2*로 정의하면 된다.
* 다른 변수의 영향을 받지 않는 편미분의 특성 상 *L1*과 *L2*는 서로에게 상수로 취급되기 때문에 *output1* 의 손실 기울기는 *y1*과의 후처리 과정을 통해 *L1*을 이용해 구하면 되고 *output2*의 손실 기울기는 *y2*와의 후처리 과정을 통해 *L2*를 이용해 구하면 된다.
* 즉, **기존의 방법 그대로를 적용해 두 부분의 손실 기울기를 따로 구하면 된다**.
* 구해진 손실 기울기는 *두 부분으로 갈라진 출력 계층에 따로 반영*되지만 *은닉 계층 퍼셉트론에 이르러서는 인접한 계층 간의 완전 연결 탓에 마구 뒤섞여 반영*된다.


## ADAM 최적화 기법 ADAM Optimizer
### 확률적 경사 하강법 Stochastic Gradient Descent
<img width="283" alt="sgd" src="https://user-images.githubusercontent.com/28593767/114334407-791f9280-9b85-11eb-82e5-635c078e5147.png">

* 기존의 경사 하강법에서 특정 데이터만을 샘플링하여 학습하는 **확률적 경사 하강법(SGD)** 은 심층 신경망(DNN) 을 학습시키기위해 주로 이용되고 있는 최적화 기법이다.
* 심층 신경망을 더 효율적으로 학습시키기 위해서 다양한 SGD의 변형이 제시되고 있는데, 이러한 SGD의 변형은 크게 두 가지로 나눠 볼 수 있다. 
    + <img width="548" alt="sgd2" src="https://user-images.githubusercontent.com/28593767/114334408-791f9280-9b85-11eb-88a2-bff434714e5c.png">
    + 첫 번째는 **momentum** 이라는 개념을 이용하여 gradient를 수정하는 알고리즘이고 두 번째는 자동으로 learning rate를 조절하는 알고리즘이다.


### Momentum
<img width="273" alt="momentum" src="https://user-images.githubusercontent.com/28593767/114334409-79b82900-9b85-11eb-962a-5caf5dd4def2.png">

* 기존의 SGD는 현재 시간의 gradient만을 이용해서 파라미터를 업데이트했지만, 반면 모멘텀 기반의 SGD는 이전 시간의 gradient까지 고려하여 파라미터를 업데이트한다.
* 이 식에서 𝛽 ∈ [0,1]은 일반적으로 0.9로 설정되는 하이퍼파라미터로서 이전 gradient와 현재의 gradient 간의 영향력을 조절하는 역할을 수행한다.
* 모멘텀은 현재 시간의 gradient가 가장 높은 중요도를 갖고 이전 시간의 gradient는 두 번째로 높은 중요도를 갖는 방식으로 이전 시간의 모든 gradient를 고려하여 방향을 설정하도록 한다.
* 모멘텀의 장점은 이전의 SGD 보다 더 빠르게 최적점으로 찾아갈 수 있다는 점과 지역 최적값(Local Minimum) 을 회피할 수 있다는 것이다.
    + 다만, 모멘텀을 이용한다 해서 무조건 빠져나갈 수 있다는 보장은 하지 못한다.

### Adaptive Gradient (AdaGrad)
<img width="755" alt="adagrad" src="https://user-images.githubusercontent.com/28593767/114334410-79b82900-9b85-11eb-995c-8b3cadf81d76.png">

* AdaGrad는최적화 과정을 효율적으로 만들기 위해 **고정된 learning rate가 아닌, 각각의 변수마다 적합한 learning rate를 자동으로 설정**하는 알고리즘이다.
* *변화가 많았던 변수들은 최적점이 근처에 있을 확률이 높다 판단하여 learning rate값을 낮춰 더욱 세밀하게 갱신*되도록 만든다.
* 반면 *변화가 적었던 변수들은 최적점에서 멀리 벗어나 있을 확률이 높기 때문에 learning rate값을 크게 함으로써 더욱 빠르게 최적점으로 수렴*하게 만든다.
* ε은 일반적으로 10e−8과 같은 매우 작은 상수로 0으로 나눠지는 것을 방지하기 위해 추가된다.
* AdaGrad의 장점은 learning rate decay와 같은 방법들을 이용하여 *학습률을 직접적으로 조절하지 않아도 된다는 점*과 모든 변수에 일괄적으로 동일한 학습률을 적용하는 기존의 SGD 기반 알고리즘과는 달리 *적합한 학습률을 자동으로 설정한다는 것*이다.
* AdaGrad의 문제점은 학습률을 조절하는 *g_ij*은 어떠한 값을 제곱한 것이 계속 더해지기 때문에 시간이 지날수록 증가하고, 이에 따라 학습률 또한 시간에 따라 감소하여 어느 정도 지난 뒤에는 학습률이 매우 작아져 가중치가 갱신되지 않는다는 점이다.

### Root Mean Square Propagation (RMSProp)
<img width="397" alt="rmsprop" src="https://user-images.githubusercontent.com/28593767/114340241-1a144a80-9b92-11eb-98cb-ea9e7b385709.png">

<img width="397" alt="rmsprop_w" src="https://user-images.githubusercontent.com/28593767/114340240-197bb400-9b92-11eb-8f90-e2453a6d5ff8.png">

* RMSProp는 학습이 진행될수록 분모에 위치하고 있는 gradient 제곱의 합인 *g_ij*의 값이 커짐으로써 학습률이 극단적으로 감소하는 AdaGrad의 문제점을 해결하기 위해 제안된 알고리즘이다.
* RMSProp에서는 *g_ij*를 gradient 제곱의 합이 아니라 gradient 제곱의 지수 이동 평균으로 정의한다.
    + 지수 이동 평균은 최근 값을 더 잘 반영하기 위해 최근의 변화량에 더 높은 가중치를 주어 계산하는 방식을 의미한다.
* 𝛽의 값은 Momentum과 마찬가지로 0과 1 사이의 값을 갖지만 일반적으로 0.9의 값으로 설정되고 ε은 일반적으로 10e−8과 같은 매우 작은 수로 설정한다.
* 이전의 AdaGrad는 현재 시간까지의 변화량의 합으로 정의되기 때문에 시간이 지날수록 증가하여 학습률이 급격하게 감소하였지만, RMSProp에서는 지수 이동 평균으로 인해 현재의 *g_ij* 가 급격하게 감소하는 현상을 방지할 수 있다.

### Adaptive Momentum Estimation (ADAM)
<img width="765" alt="adam" src="https://user-images.githubusercontent.com/28593767/114340237-18e31d80-9b92-11eb-996a-3f84ea1e3ca2.png">

* ADAM은 딥러닝 학습에 가장 광범위하게 이용되고 있는 알고리즘으로 경험적 근거에 의하여 가장 좋은 학습 성능을 보여준 최적화 기법이다.
* 앞서 살펴보았던 두 개의 알고리즘인 Momemtum과 RMSPropp를 합친 것 같은 알고리즘이라고 할 수 있다.
* 수식을 통해 살펴보면 *g_ij*를 통해 학습률을 조절하는 RMSProp의 방식이 눈에 들어오지만, *v*를 구하는 과정에서는 Momentum의 방식이 쓰인다.
* *g*와 *v*는 지수 이동 평균으로 구해지고 𝛽1, 𝛽2 은 0과 1 사이의 값을 갖지만 일반적으로 0.9, 0.999로 설정된다.
* ADAM에서는 v_ij와 g_ij 값을 그대로 이용하지 않는다. 
    + 논문에 의하면 두 값에 대한 초기값을 0 벡터로 주면, 학습 초기에 가중치들이 0으로 편향 되는 경향을 보이고 특히 decay rate가 작으면 즉 𝛽1 ,𝛽2 가 1에 가까워지면 편향은 더 심해진다고 한다.
    + <img width="719" alt="adam2" src="https://user-images.githubusercontent.com/28593767/114340231-1680c380-9b92-11eb-90a4-924301e95940.png">



