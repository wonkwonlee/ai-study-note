# 강화 학습 Reinforcement Learning

## 강화 학습의 정의 Definition of RL
* 강화 학습이란 쉽게 말해서 ***시행 착오(Trial and Error)** 를 통해 발전해 나가는 과정*이라고 할 수 있다.
* 즉, 강화 학습이란 ***순차적 의사결정(Sequential Decision Making)** 문제에서 누적 보상을 최대화 하기 위해 시행 착오를 통해 행동을 교정하는 학습 과정*이라고 정의할 수 있다.


## 순차적 의사결정 Sequential Decision Making
* 순차적 의사결정 문제란 각 상황에 따라 하는 행동이 다음 상황에 영향을 주며 결국 연이은 행동을 잘 선택해야 하는 문제를 의미한다.
* 즉, 강화 학습은 이러한 중요한 문제를 풀기 위해 고안된 방법론이라고 할 수 있다.


## 보상 Reward
* 보상이란 의사결정을 얼마나 잘하고 있는지 알려주는 신호이다.
* 강화 학습의 목적은 과정에서 받는 보상의 총합, 즉 **누적 보상(Cumulative Reward)** 을 최대화 하는 것이라고 할 수 있다.
* 보상의 3가지 특성: ***어떻게가 아닌 얼마나, 스칼라, 희소하고 지연된 보상***

1. **어떻게 X 얼마나 O**
    + 보상의 *첫 번째 특징은 보상은 어떻게에 대한 정보를 담고 있지 않다는 점*이다.
    + 보상은 내가 어떤 행동을 하면 그것에 대해 *단순히 얼마나 잘 하고 있는지 평가 해줄 뿐, 어떻게 해야 높은 보상을 얻을 수 있을지 알려주지 않는다*.
    + 다시 말해, 보상은 지도 학습의 정답과 질적으로 다르다.
    + 보상은 어떻게 해야 할지를 직접적으로 알려주지는 않지만, 사후적으로 보상이 낮았던 행동들은 덜 하고, 보상이 높았던 행동들은 더 하면서 보상을 최대화 하도록 행동을 조금씩 수정해 나간다.

2. **스칼라**
    + 보상의 두 번째 특징은 보상이 **스칼라(Scalar)** 라는 점이다.
    + 스칼라는 **벡터(Vector)** 와는 다르게 크기를 나타내는 값 하나로 이루어져 있다. 
        - *보상이 벡터라면 동시에 2개 이상의 값을 목표로 할 수 있겠지만 스칼라이기 때문에 오직 하나의 목적만을 가져야 한다*.   
    + 강화 학습은 스칼라 형태의 보상이 있는 경우에만 적용할 수 있다.
        - 만일 어떤 문제가 하나의 목표만을 설정하기 어렵다면 문제를 단순화하여 하나의 목표를 설정해야 한다.
        - 반대로 잘 정해진 하나의 숫자로 된 목표가 있다면 강화 학습은 해당 목표를 최대로 취하도록 최적화한다. 

3. **희소하고 지연된 보상**
    + 보상의 세 번째 특징은 보상이 **희소(Sparse)** 할 수 있으며 또 **지연(Delay)** 될 수 있다는 점이다.
    + 행동과 보상이 일대일로 대응이 된다면 행동에 대한 평가가 즉각적으로 이루어지는 만큼 강화 학습이 한결 쉬워진다.
    + 하지만 *보상은 선택했던 행동의 빈도에 비해 훨씬 가끔 주어지거나, 행동이 발생한 후 한참 뒤에 나올 수 있고 이 때문에 행동과 보상의 연결이 어려워 진다.*
    + 다시 말해, *보상이 어떤 행동 덕분인지 책임 소재가 불분명하기 때문에 그만큼 학습 난이도가 높다*.
    + 보상이 희소할수록 학습이 어려워지고 이러한 문제를 해결하기 위해 최근 강화 학습 연구에서는 **밸류 네트워크(Value Network)** 등의 아이디어가 등장했다.
    + 이러한 특성은 *지도 학습과 강화 학습을 본질적으로 다르게 만드는 요소*라고 할 수 있다.
        - 지도 학습에서는 정답이 뒤늦게 주어지는 경우는 없고 오직 데이터와 정답의 쌍이 존재한다.
        - 반면 강화 학습에서 다루는 문제는 순차적 의사결정 문제이므로 시간에 따른 흐름이 중요하고 이 흐름에서 보상이 뒤늦게 주어질 수 있다.


## 순차적 행동 결정문제 Markov Decision Process
1. **상태 State**
    + *에이전트의 상태로서 공학에서 많이 사용하는 개념*으로 정적인 요소 뿐 아니라 동적인 요소 또한 상태로 표현할 수 있다.
    + 에이전트가 *상태를 통해 상황을 판단해서 행동을 결정해야 하기에 에이전트에게 충분한 정보를 제공해야* 하므로 엄밀히 말하면 상태보다는 관찰이라는 것이 정확한 표현이라고 할 수 있다.
2. **행동 Action**
    + 에이전트가 어떠한 상태에서 취할 수 있는 행동으로 [상, 하, 좌, 우]와 같은 것을 말한다. 
    + *학습이 되지 않은 에이전트는 어떤 행동이 좋은 행동인지에 대한 정보가 전혀 없으므로 처음에는 무작위로 행동*을 취할 수 밖에 없다. 
    + 하지만 에이전트는 학습하면서 *특정한 행동들을 할 확률을 높이고 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다*.
3. **보상 Reward**
    + 보상은 강화학습을 다른 머신러닝 기법과 다르게 만들어 주는 가장 핵심적인 요소이다.
    + 보상이라는 정보를 통해 에이전트는 자신이 했던 행동들을 평가할 수 있고 이로 인해 어떤 행동이 좋은 행동인지 알 수 있다.
    + 강화 학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것이다.
4. **정책 Policy**
    + *순차적 행동 결정문제에서 구해야 할 답은 바로 정책*이다.
    + 에이전트가 행동을 하려면 *특정 상태가 아닌 모든 상태에 대해 어떤 행동을 해야 할지 알아야 하는데 이렇게 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것이 정책*이다.
    + *순차적 행동 결정문제를 풀었다고 한다면 제일 좋은 정책을 에이전트가 얻었다는 것*으로 여기서 제일 좋은 정책은 **최적 정책(Optimal Policy)** 이라고 하며 *에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대*로 받을 수 있습니다.

> 강화 학습은 문제의 정의를 어떻게 설정하느냐에 따라 학습을 잘하는 지가 결정되고 적절한 보상을 받으며 에이전트가 학습할 수 있게 하는 것이 중요하다. 따라서 에이전트가 판단하기에 충분한 정보를 얻을 수 있도록 순차적 행동 결정 문제를 정의해야 한다.


## 아타리 브레이크 아웃 Atari Breakout
* 딥마인드는 *"Playing Atari with Deep Reinforcement Learning"* 논문을 통해 브레이크아웃(Breakout) 게임에서 MDP를 어떻게 구성할지 그리고 에이전트는 어떻게 학습할지를 발표했다.

1. MDP
![mdp](https://user-images.githubusercontent.com/28593767/116837612-21190080-ac06-11eb-9375-cc79e00ef5d7.png)

* 상태
    + 브레이크아웃에서 에이전트가 환경으로부터 받아들이는 상태는 게임 화면이다.
    + 에이전트가 상황을 파악할 수 있도록 같은 화면을 연속으로 4개를 받아 이 4개의 화면이 하나의 상태로 에이전트에게 제공된다.
    + 만일 현재의 이미지 하나만 입력으로 받는다면 게임 화면이 어떤 상황인지 알 수 없으므로 공이 어느 방향으로 움직이는지에 대한 정보가 필요하므로 4개의 연속된 이미지를 입력으로 받는다.
* 행동
    + 브레이크아웃에서는 제자리, 왼쪽, 오른쪽, 발사가 가능하고 발사는 게임을 시작할 때 사용한다. 
    + 즉 에이전트가 게임 도중에 취할 수 있는 행동은 발사를 제외한 3가지로 볼 수 있다.
* 보상
    + 벽돌이 깨질 때마다 보상을 +1점씩 받고 더 위쪽을 깰수록 더 큰 보상을 받는다.
    + 아무것도 깨지 않을 때는 보상으로 0을 받는다.
    + 공을 놓쳐서 목숨을 잃을 경우에 보상으로 -1을 받는다.

2. 학습
![train](https://user-images.githubusercontent.com/28593767/116837613-22e2c400-ac06-11eb-84b0-327ba9dad2ab.png)

* 처음에 에이전트는 게임이나 상황에 대해 전혀 모르고 무작위로 제자리, 왼쪽, 오른쪽으로 움직인다.
* 그러다가 에이전트가 우연히 벽돌을 깨면 게임의 환경으로부터 +1의 보상을 받고 공을 놓친다면 -1의 보상을 받는다.
* 이러한 상황을 반복하여 에이전트는 게임을 하면서 어떻게 해야 공을 떨어뜨리지 않고 벽을 깰 수 있는지 학습할 수 있다.
* 강화 학습을 통해 학습되는 것은 인공신경망으로 *입력으로 앞서 살펴본 4개의 연속적인 게임 화면이 들어오면 인공신경망으로 그 상태에서 에이전트가 할 수 있는 각 행동이 얼마나 좋은지 출력*으로 내놓는다. 
    + 행동이 얼마나 좋은지가 행동의 가치가 되고 이것을 **Q Function** 이라고 한다.
    + 이 때 사용된 인공신경망을 **DQN(Deep Q-Network)** 라고 한다.
    + 에이전트는 DQN이 출력한 큐함수를 보고 큰 가치를 지니는 행동을 선택하고 환경은 에이전트에게 보상과 다음 상태를 알려준다.
    + 에이전트는 이렇게 환경과 상호작용하면서 DQN을 더 많은 보상을 받도록 조금씩 조정한다.

### 병렬성 Parallelism
* 강화 학습은 경험을 쌓는 부분(시뮬레이터)의 병렬성을 쉽게 증가시킬 수 있다. 
* 혼자서 혼자서 시행착오를 통해서 배운다면 한참 걸리겠지만 무수히 늘어난 병렬성의 힘 아래에서는 어려운 지식도 빠르게 습득할 수 있다. 
* OpenAI가 Dota2에 강화 학습을 적용할 때에는 시뮬레이션을 위해 256개의 GPU와 12만 8천 개의 CPU 코어가 사용되었다고 한다. 
* 알파고 역시 같은 방식으로 학습되어 이겼던 경기에서 뒀던 수들을 좀 더 자주 두도록 하고 졌던 경기에서 뒀던 수들은 덜 두도록 조정되었다.

### 자가 학습 Self-Learning
* 강화 학습은 그냥 시뮬레이션 환경 속에 던져 놓고 달성해야할 목적 만을 알려주며 알아서 배우게 한다. 
* 그러면 에이전트는 수 없이 많은 시행착오를 겪어가며 목적을 달성할 방법을 깨닫는다.
* 알파고 또한 학습 초기에는 프로 바둑기사들의 기보를 통해 지도 학습을 진행했지만 이후 자가 학습에 기반을 둔 강화 학습을 통해 사람을 뛰어넘을 수 있었다.


## MDP의 구성 요소
<img width="1175" alt="mdp2" src="https://user-images.githubusercontent.com/28593767/116837614-237b5a80-ac06-11eb-9b14-707d34bf5d8a.png">

*  MDP를 통해 정의된 문제에서 에이전트가 학습하기 위해 가치 함수라는 개념을 도입하는데, 이 개념은 **벨만 방정식(Bellman Equation)** 과 연결된다.
* MDP는 상태와 행동을 포함한 보상 함수, 상태 변환 확률, 감가율(할인율)로 구성되어 있다.
1. **상태**
    + <img width="498" alt="state" src="https://user-images.githubusercontent.com/28593767/116837621-25ddb480-ac06-11eb-88f2-b35253e31a5b.png">
    + **S는 에이전트가 관찰 가능한 상태의 집합**으로 상태란 *자신의 상황에 대한 관찰*이라고 할 수 있다.
    + MDP에서 상태는 시간에 따라 확률적으로 변하고 *시간 t에서의 상태 𝑆_t가 어떤 상태 s다*라고 표현한다.
2. **행동**
    + **에이전트가 상태 S_t에서 할 수 있는 가능한 행동의 집합은 A**라고 표현한다.
    + 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같으므로 하나의 집합 A로 나타내고 어떤 특정한 행동은 a라고 표현한다.
    + 에이전트가 특정 행동을 했을 때 어디로 이동할지 결정하는 것이 상태 변환 확률이다.
3. **보상 함수**
    + <img width="358" alt="reward" src="https://user-images.githubusercontent.com/28593767/116837619-25451e00-ac06-11eb-975d-72a7d7bdc089.png">
    + 보상은 **에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보**이다.
    + 위 보상 함수는 시간 t일때 상태가 S_t = s이고 그 상태에서 행동 A_t = a를 했을 경우에 받을 보상에 대한 기댓값 E를 나타난 함수이다.
        - 기댓값은 *어떤 정확한 값이 아니라 나오게 될 숫자에 대한 예상*이다.
        - 환경에 따라서 *같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수도* 있기 때문에 보상 함수를 기댓값으로 표현한다.
    + 보상 함수에서 중요한 점은 에이전트가 어떤 상태에서 행동한 것은 시간 t이지만 보상을 받는 것은 t+1인데 이는 보상을 에이전트가 알고 있는 것이 아니고 환경이 알려주는 것이기 때문이다. 
    + 에이전트가 상태 s에서 행동 a를 하면 환경은 에이전트가 가게 되는 다음 상태 s'과 에이전트가 받을 보상을 에이전트에게 알려준다.
4. **상태 변환 확률**
    + <img width="343" alt="prob" src="https://user-images.githubusercontent.com/28593767/116837620-25451e00-ac06-11eb-8378-60edfe78f540.png">
    + 상태의 변화에는 확률적인 요인이 들어가는데 이를 수식으로 표현한 것이 상태 변환 확률이다.
    + **상태 변환 확률은 상태 s에서 행동 a를 취했을 때 다른 상태 s'에 도달할 확률**로 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부이다.
    + 상태 변환 확률은 환경의 **모델(Model)** 이라고도 하는데, 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려준다.
5. **감가율(할인율)**
    + <img width="421" alt="discount" src="https://user-images.githubusercontent.com/28593767/116837615-2413f100-ac06-11eb-9708-eaaa328b33ca.png">
    + 에이전트가 항상 현재에 판단을 내리기 때문에 현재에 가까운 보상일수록 더 큰 가치를 가진다.
    + **에이전트는 그 보상이 얼마나 시간이 지나서 받는지를 고려해서 현재의 가치로 따지므로 감가율이란 미래를 평가 절하하는 항**이라고 할 수 있다.
    + *현재와 미래 사이에는 다양한 확률적 요소들이 있고 이로 인해 당장 느끼는 가치에 비해 미래에 느끼는 가치가 달라질 수 있으므로 미래의 가치에는 불확실성을 반영하고자 감쇠*를 해준다.
6. **정책**
    + <img width="304" alt="policy" src="https://user-images.githubusercontent.com/28593767/116837731-94227700-ac06-11eb-874a-7b70f6f0a0d8.png">
    + 정책은 모든 상태에서 에이전트가 할 행동으로 **상태가 입력으로 돌아오면 해야할 행동을 출력으로 내보내는 일종의 함수**라고 할 수 있다.
    + 정책은 각 상태에서 단 하나의 행동 만을 나타낼 수도 있고 확률적으로 a1 = 10%, a2 = 90%와 같이 나타낼 수도 있다.
    + 에이전트의 최적 정책은 각 상태에서 단 하나의 행동만을 선택하지만 *에이전트가 학습하고 있을 때는 정책이 하나의 행동만을 선택하기 보다는 확률적으로 여러 개의 행동을 선택할 수 있어야 한다*.
    + 정책의 수식은 시간 t에 𝑆_t = s에 에이전트가 있을 때 가능한 행동 중에서 A_t = a를 할 확률을 나타낸다.
    + 즉, *최적 정책을 얻기 위해서 현재의 정책보다 더 좋은 정책을 학습해나가야 하는것이 강화학습*이다.

<img width="574" alt="rl" src="https://user-images.githubusercontent.com/28593767/116837728-9258b380-ac06-11eb-823a-f7f911d599dc.png">

> 에이전트는 현재 상태에서 앞으로 받을 보상들을 고려해서 행동을 결정하고 환경은 에이전트에게 실제 보상과 다음 상태를 알려준다. 이러한 과정을 반복하면서 초기 에이전트는 어떤 상태에서 앞으로 받을 것이라 예상했던 보상에 대해 틀렸다는 것을 알게 된다. 이 때 앞으로 받을 것이라 예상하는 보상을 가치 함수라고 하며 에이전트는 실제로 받은 보상을 토대로 가치 함수와 정책을 바꿔 나간다. 이러한 학습과정을 충분히 반복한다면 가장 많은 보상을 받게 하는 최적 정책을 학습할 수 있다.
