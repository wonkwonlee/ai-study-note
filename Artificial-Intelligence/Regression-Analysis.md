# 회귀 분석 Regression Analysis

## 신경망의 세가지 기본 출력 유형
![three](https://user-images.githubusercontent.com/28593767/112080349-304b6e00-8bc5-11eb-98d4-ae2dbc1929de.png)

1. 회귀 분석 : 어떤 특징값 하나를 숫자로 추정하여 출력
2. 이진 판단 : '예'나 '아니오' 가운데 한 쪽을 택해 출력
3. 선택 분류 : 몇 가지 후보 중 하나를 골라 선택 결과를 출력


## 회귀 Regression
* 회귀는 옛날 상태로 돌아간다는 의미이다.
* 통계학에서는 연속형 변수 사이의 모형을 구한 뒤 적합성을 측정하는 분석 방법을 말한다.
    + 다시 말해, 입력값들을 근거로 미지의 변숫값을 추정하고 예측하는데 이용된다.
* 딥러닝 알고리즘에서의 값 추정 또한 신경망 모델이 입력 데이터를 근거로 출력값을 추정하는 것이다.

![problem](https://user-images.githubusercontent.com/28593767/112080345-2f1a4100-8bc5-11eb-95c2-cbf5089d817b.png)

* 머신러닝을 적용하기 위해서는 *y = Θ_0 + Θ_1x* 형태의 함수를 만들어야 한다.
    + 세타(*Θ*)는 구해야 할 미지수, 기울기와 편향을 나타낸다.
* 이러한 매개변수 값을 적절한 값으로 설정하기 위해 머신러닝이 활용된다.
    + 학습 데이터를 기존에 작성에 작성한 함수에 대입하여 세타를 정한다.


## 최소 제곱법 Least Squares Method
![lsm_d](https://user-images.githubusercontent.com/28593767/112080347-2fb2d780-8bc5-11eb-9ca3-e02280b33464.png)

* *y* 와 *fΘ(x)* 의 오차가 0이 되는 것이 가장 이상적이지만 그래프에 있는 모든 점에 생기는 오차를 0으로 만드는 것은 불가능하다.
* 따라서 **모든 점에서 생기는 오차의 합계가 가능한 작아지도록** 한다.

![lsm](https://user-images.githubusercontent.com/28593767/112080351-30e40480-8bc5-11eb-99d5-f24b0872e0cf.png)

* 목적함수 *E(Θ)* 로서, E는 Error의 머릿글자이고 *x^(i), y^(i)* 는 i번째 학습 데이터를 의미한다.
* 각각의 학습 데이터마다 생기는 오차를 제곱하여 모두 더하고 그 값에 1/2을 곱한다.
    + 양수와 음수가 섞여 있는 경우 계산이 어려워지기에 오차가 양수가 되도록 제곱을 수행한다.
    + 또한 제곱의 효과는 오차가 큰 값에 대해서 더 큰 오차값을 부여한다.
    + 전체에 1/2을 곱하는 이유는 결과로 나온 식을 간단한 모양으로 만들기 위해 붙인 상수이다.
    + 최적화 문제에 상수를 붙여도 최솟값이 존재하는 위치는 변하지 않는다.
    + ![square](https://user-images.githubusercontent.com/28593767/112080354-30e40480-8bc5-11eb-831a-4079d2d9e3e9.png)
* 이렇게 **계산된 *E(Θ)* 값이 가장 작아지게 되는 Θ값을 찾는 것이 목적**이고 이것을 **최적화 문제**라고 한다.
    + Adam 알고리즘 : 딥러닝 옵티마이저
* 목적함수 *E(Θ)* 이 작아진다는 의미는 오차가 작아진다는 의미이고 이러한 기법을 **최소 제곱법**이라고 한다.

    
## 경사 하강법 Gradient Descent
* 계산한 목적함수 *E(Θ)* 를 감소시켜야 하기 위해서는 매개변수 *Θ*를 적절하게 조절해야 한다. 
* 하지만 기준값을 비교해 가며 계산하는 방식은 효율적이지 못하다.
* 따라서 *변화하는 정도를 구하기 위해 사용되는 미분*을 활용한다.

![gd](https://user-images.githubusercontent.com/28593767/112080356-317c9b00-8bc5-11eb-831b-4e22672f5ccf.png)

* **𝜂(에타)** 는 **학습률(Learning rate)** 이라 하는 양의 정수로 이 *학습률의 크기에 따라 최솟값에 도달하기까지 갱신해야 하는 횟수가 달라지게 됩니다.* 
* 즉 학습률로 인해 **최솟값에 수렴되는 속도가 달라진다**는 겁니다.
* 학습률이 *적절한 값을 가지면 조금씩 최솟값을 향하여 x값이 줄어들게 된다.*
* 반면 학습률이 너무 크면 x값이 줄어들지 않게 되거나 혹은 최솟값에서 멀어지게 되는데 이것을 **발산**이라 한다.
* 또한 학습률이 너무 작으면 최솟값으로 이동하는 보폭이 작아져 갱신횟수가 늘어나게 된다.

![gd_d](https://user-images.githubusercontent.com/28593767/112080342-2cb7e700-8bc5-11eb-89a3-179d0225a638.png)

### 목적함수의 미분
![pde](https://user-images.githubusercontent.com/28593767/112085079-36454d00-8bcd-11eb-9c07-5109d23b2054.png)

* 목적함수의 *fΘ(x)* 는 ***Θ_0(편향), Θ_1(가중치)*** 의 두 매개 변수를 가지고 있는 **2변수 함수**이다.
* 따라서 매개변수 갱신식은 편미분으로 나타낼 수 있다.
    + ![pde_eq](https://user-images.githubusercontent.com/28593767/112085087-38a7a700-8bcd-11eb-9402-111e1447656b.png)
* 최종적으로 매개변수 *Θ_0, Θ_1*의 갱신식은 다음과 같다.
    + ![theta](https://user-images.githubusercontent.com/28593767/112085089-39403d80-8bcd-11eb-8ffe-c27f7c1b9cf9.png)
* 이 식에 따라 매개변수를 갱신해가면 알맞은 1차함수 *fΘ(x)* 를 발견할 수 있고 임의의 x값에 대한 출력값을 예측할 수 있게 된다.


## 다항식 회귀 Polynomial Regression
* 함수의 모양이 일직선이 아닌 곡선의 형태가 더 필요한 경우 *fΘ(x)* 를 2차함수 혹은 더 큰 차수의 식으
로 정의하면 곡선의 형태를 그려낼 수 있다.
    + ***fΘ(x) = Θ_0 + Θ_1x +Θ_2x^2***
* 하지만 무작정 차수를 늘려가는 방식은 **과잉적합(Overfitting)** 문제가 발생할 수 있다.
* 결과적으로 매개변수가 *Θ_3, Θ_4,...* 이렇게 늘어남에 따라 같은 방법으로 갱신식을 구해줄 수 있고 이러한 **다항식의 차수를 늘린 함수를 사용하는 것**을 **다항식 회귀(Polynomial regression)** 라고 한다.
* 다항식의 매개변수 갱신식은 다음과 같다.
    + ![poly](https://user-images.githubusercontent.com/28593767/112085502-00549880-8bce-11eb-8f7c-58f80680da0c.png)

## 다중 회귀 Multiple Regression
* 변수가 두 개 이상인 경우 *fΘ(x_1,x_2,x_3) = Θ_0 + Θ_1x_1 + Θ_2x_2 + Θ_3x_* 로 정의할 수 있다.
* 변수가 n개인 경우 일반화 하여 다음과 같이 나타낸다.
    + ![fn](https://user-images.githubusercontent.com/28593767/112085501-ffbc0200-8bcd-11eb-8dd1-1c9ebabc2a8d.png)
* 매번 n개의 x를 쓰는 대신 매개변수 *Θ_n*과 변수 *x_n*을 벡터 형태로 표기한다.
* 일반적으로 벡터끼리 곱할 경우에는 이렇게 한쪽을 전치(Transpose) 한 후에 곱셈을 수행하여 내적을 취한다.
    + ![vec_c](https://user-images.githubusercontent.com/28593767/112085500-fe8ad500-8bcd-11eb-9cd3-26963a9a9429.png)
* 다중 회귀의 매개변수 갱신식 역시 *u = E(Θ), v = fΘ(x)* 로 두는 것은 같지만 변수가 많아짐에 따라 일반화하여 j번째 요소인 *Θ_j*로 편미분해야 한다.
    + ![pde_j](https://user-images.githubusercontent.com/28593767/112085505-00ed2f00-8bce-11eb-9c0c-3274981f825e.png)
* j번째 매개변수의 갱신식은 다음과 같다.
    + ![theta_j](https://user-images.githubusercontent.com/28593767/112085507-00ed2f00-8bce-11eb-870c-9444c25710bb.png)
* 이렇게 여러 개의 변수를 사용하는 것을 **다중회귀(Multiple Regression)** 라고 한다.
* 다만 경사 하강법은 모든 학습 데이터의 개수만큼 반복해서 계산해야 한다는 점 때문에 학습 데이터가 많으면 많을수록 학습에 대한 시간을 많이 소모하게 된다는 단점이 있다.
    + 이러한 경우를 극복하고자 이후 더 효율적인 다양한 기법이 등장한다.


