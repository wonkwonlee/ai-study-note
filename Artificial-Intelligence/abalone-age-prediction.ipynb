{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerical-volunteer",
   "metadata": {},
   "source": [
    "# Abalone Age Prediction\n",
    "\n",
    "![method](https://user-images.githubusercontent.com/28593767/112245885-1e390080-8c95-11eb-9dee-5f66bb94cbf8.png)\n",
    "\n",
    "회귀 분석을 이용한 전복 나이 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-wallet",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brown-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exact-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyper parameters\n",
    "RAND_MEAN = 0\n",
    "RAND_STD = 0.0030\n",
    "\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement main function\n",
    "# epoch number, minibatch size, output report, training rate\n",
    "def main_exec(epoch_count=10, mb_size=10, report=1, train_rate=0.8):\n",
    "    load_dataset()   # Load data\n",
    "    init_model()     # Initialize parameters\n",
    "    train_and_test(epoch_count, mb_size, report, train_rate)   # Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement loading dataset function\n",
    "def load_dataset():\n",
    "    with open('data_nn/abalone.csv') as csv_file:\n",
    "        csvreader = csv.reader(csv_file)\n",
    "        next(csvreader, None)    # Skip the first row (column info)\n",
    "        rows = []\n",
    "        # Store csv data to empty list, rows\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "    # Global variable (전역 변수)\n",
    "    # Input vector size increases from 8 to 10 (One-hot vector)\n",
    "    global data, input_cnt, output_cnt\n",
    "    input_cnt, output_cnt = 10, 1    # Size of independant and dependant variables\n",
    "    data = np.zeros([len(rows), input_cnt+output_cnt])   # Buffer\n",
    "    \n",
    "    # One-hot vector\n",
    "    # I = [1,0,0], M = [0,1,0], F = [0,0,1]\n",
    "    for n, row in enumerate(rows):\n",
    "        if row[0] == 'I': data[n, 0] = 1\n",
    "        if row[0] == 'M': data[n, 1] = 1\n",
    "        if row[0] == 'F': data[n, 2] = 1\n",
    "        data[n, 3:] = row[1:]    # For the rest, store data from enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement parameters initializing function\n",
    "\n",
    "def init_model():\n",
    "    global weight, bias, input_cnt, output_cnt\n",
    "    weight = np.random.normal(RAND_MEAN, RAND_STD, [input_cnt, output_cnt])\n",
    "    bias = np.zeros([output_cnt])\n",
    "    # print(weight)\n",
    "    # print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.      1.      0.     ...  0.101   0.15   15.    ]\n",
      " [ 0.      1.      0.     ...  0.0485  0.07    7.    ]\n",
      " [ 0.      0.      1.     ...  0.1415  0.21    9.    ]\n",
      " ...\n",
      " [ 0.      1.      0.     ...  0.2875  0.308   9.    ]\n",
      " [ 0.      0.      1.     ...  0.261   0.296  10.    ]\n",
      " [ 0.      1.      0.     ...  0.3765  0.495  12.    ]]\n"
     ]
    }
   ],
   "source": [
    "# data 출력 형태 확인 (원 핫 벡터 확인)\n",
    "\n",
    "with open('data_nn/abalone.csv') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader, None)    # Skip the first row (column info)\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "        \n",
    "global data\n",
    "data = np.zeros([len(rows), 11]) \n",
    "\n",
    "for n, row in enumerate(rows):\n",
    "    if row[0] == 'I': data[n, 0] = 1\n",
    "    if row[0] == 'M': data[n, 1] = 1\n",
    "    if row[0] == 'F': data[n, 2] = 1\n",
    "    data[n, 3:] = row[1:]  \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training and testing function & Ouput training result\n",
    "\n",
    "def train_and_test(epoch_count, mb_size, report, train_rate):\n",
    "    step_count = arrange_data(mb_size, train_rate)  # Return how many steps in each minibatch\n",
    "    test_x, test_y = get_test_data()                # Get x and y value from test data\n",
    "    \n",
    "    # Nested for-loop\n",
    "    for epoch in range(epoch_count):\n",
    "        losses, accs = [], []         # Store loss and accuracy of total minibatch (1 epoch)\n",
    "        for n in range(step_count):\n",
    "            # Return x and y value from train data from minibatch size and step count\n",
    "            train_x, train_y = get_train_data(mb_size, n)\n",
    "            loss, acc = run_train(train_x, train_y)\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "        \n",
    "        if report > 0 and (epoch+1) % report == 0:\n",
    "            acc = run_test(test_x, test_y)\n",
    "            # format 5.3f : 소수점을 포함한 전체 자릿수.소수점 이하 자릿수\n",
    "            print(\"Epoch {} : Train - loss = {:5.3f}. accuracy = {:5.3f} / Test = {:5.3f}\".\\\n",
    "                  format(epoch+1, np.mean(losses), np.mean(accs), acc))\n",
    "            \n",
    "    final_acc = run_test(test_x, test_y)\n",
    "    print(\"\\n 최종 테스트 결과 : final accuracy = {:5.3f}.format(final_acc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "needed-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터의 수(행) : 4177\n",
      "데이터의 70%의 미니배치 스텝 수 : 29\n"
     ]
    }
   ],
   "source": [
    "print(\"총 데이터의 수(행) :\", data.shape[0])\n",
    "mb_size = 100\n",
    "step_count = int(data.shape[0] * 0.7) // mb_size\n",
    "print(\"데이터의 70%의 미니배치 스텝 수 :\", step_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-greenhouse",
   "metadata": {},
   "source": [
    "## Arranging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solar-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement arranging data function\n",
    "\n",
    "def arrange_data(mb_size, train_rate):\n",
    "    global data, shuffle_map, test_begin_index\n",
    "    # Randomly shuffle the data\n",
    "    shuffle_map = np.arange(data.shape[0])\n",
    "    np.random.shuffle(shuffle_map)\n",
    "    # Get minibatch step count\n",
    "    step_count = int(data.shape[0] * train_rate) // mb_size\n",
    "    \n",
    "    # Get training and testing data boundatry index\n",
    "    test_begin_index = step_count * mb_size\n",
    "    \n",
    "    return step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blank-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dividing train data function\n",
    "def get_train_data(mb_size, nth):\n",
    "    global data, shuffle_map, test_begin_index, output_cnt\n",
    "    if nth == 0 :\n",
    "        np.random.shuffle(shuffle_map[:test_begin_index])\n",
    "    train_data = data[shuffle_map[mb_size * nth : mb_size * (nth+1)]]\n",
    "    \n",
    "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]\n",
    "\n",
    "# Implement dividing train data function test data function\n",
    "def get_test_data():\n",
    "    global data, shuffle_map, test_begin_index, output_cnt\n",
    "    test_data = data[shuffle_map[test_begin_index:]]\n",
    "    \n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "curious-discrimination",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffle_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1c69deb69bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mshuffle_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnth\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmb_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffle_map' is not defined"
     ]
    }
   ],
   "source": [
    "nth = 0\n",
    "mb_size = 100\n",
    "\n",
    "global shuffle_map\n",
    "train_data = data[shuffle_map[mb_size * nth : mb_size * (nth+1)]] \n",
    "print(train_data.shape)\n",
    "\n",
    "print(\"---\"*20)\n",
    "for n, i in enumerate(train_data[0:5]):\n",
    "    print(n,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "helpful-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경계 인덱스 생성 :  3300\n",
      "일반적인 순서 \n",
      " [   0    1    2 ... 4174 4175 4176]\n",
      "처음부터 경계선까지의 순서 셔플 \n",
      " [2852 2325 3253 ... 4174 4175 4176]\n",
      "3295번째부터 3305번째까지의 순서 출력 \n",
      " [1693 2119 2982 2863 2151 3300 3301 3302 3303 3304]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "shuffle_map = np.arange(data.shape[0])\n",
    "# Get minibatch step count\n",
    "step_count = int(data.shape[0] * 0.8) // mb_size \n",
    "# Get training data and testing data boundary index\n",
    "test_begin_index = step_count * mb_size\n",
    "# Print boundary index\n",
    "print(\"경계 인덱스 생성 : \", test_begin_index) \n",
    "# Print regular order\n",
    "print(\"일반적인 순서 \\n\", shuffle_map) \n",
    "np.random.shuffle(shuffle_map[:test_begin_index]) \n",
    "# Print shuffled order till boundary index\n",
    "# Print shuffled data from 0 to 3300 and regular data after\n",
    "print(\"처음부터 경계선까지의 순서 셔플 \\n\", shuffle_map) \n",
    "# Print shuffled data from 3295 to 3300 and regular data from 3300 to 3395\n",
    "print(\"3295번째부터 3305번째까지의 순서 출력 \\n\", shuffle_map[3295:3305]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-digest",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "run_train()은 학습을 수행하는 과정이다.\n",
    "\n",
    "1. 순전파 과정을 통해 얻은 예측에 대한 손실 구하기\n",
    "    * forward_neuralnet(), forward_postproc()\n",
    "2. 예측된 값을 바탕으로 정확도 산출하기\n",
    "    * eval_accuracy()\n",
    "3. 손실이 나오기까지 영향을 미친모든 요소에 대한 기울기 구하기\n",
    "    * backprop_postproc(), backprop_neuralnet()\n",
    "    \n",
    "이렇게 구한 결괏값을 학습률과 곱해 기존 파라미터에서 빼주는 학습단계로 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consolidated-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement calculating output from matrix multiplication\n",
    "def forward_neuralnet(x):\n",
    "    global weight\n",
    "    # matmul() : multiply matrices\n",
    "    output = np.matmul(x, weight) + bias\n",
    "    # Return output and x. x will be used for backpropagation (aux_nn)\n",
    "    return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abroad-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement processing forward propagation and calculating MSE function\n",
    "def forward_postproc(output, y):\n",
    "    # Mean value of squared difference\n",
    "    diff = output - y\n",
    "    square = np.square(diff)\n",
    "    loss = np.mean(square)\n",
    "    \n",
    "    return loss, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chemical-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation of neural net(regression) function\n",
    "def eval_accuracy(output, y):\n",
    "    mdiff = np.mean(np.abs(output - y) / y)\n",
    "    return 1 - mdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surface-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement actual training and actual testing function\n",
    "def run_train(x, y): \n",
    "    output, aux_nn = forward_neuralnet(x)        # Calculate neural net output\n",
    "    loss, aux_pp = forward_postproc(output, y)   # Calculate loss function\n",
    "    accuracy = eval_accuracy(output, y)          # First step of training\n",
    "    \n",
    "    G_loss = 1.0                                 # Second setep of training\n",
    "    G_outout = backprop_postproc(G_loss, aux_pp) # Get output gradient from back propagation\n",
    "    backprop_neuralnet(G_outout, aux_nn)         # Update output gradient \n",
    "    \n",
    "    return loss, accuracy\n",
    "    \n",
    "    \n",
    "def run_test(x, y):\n",
    "    output, _ = forward_neuralnet(x)\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "synthetic-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement processing back propagation function\n",
    "def backprop_postproc(G_loss, diff):\n",
    "    shape = diff.shape\n",
    "    \n",
    "    g_loss_square = np.ones(shape) / np.prod(shape)\n",
    "    g_square_diff = diff * 2\n",
    "    g_diff_output = 1\n",
    "    \n",
    "    G_square = g_square_diff * G_loss\n",
    "    G_diff = g_square_diff * G_square\n",
    "    G_output = g_diff_output * G_diff\n",
    "    \n",
    "    return G_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stunning-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement back propagation function\n",
    "def backprop_neuralnet(G_output, x):\n",
    "    global weight, bias\n",
    "    g_output_w = x.transpose()\n",
    "    \n",
    "    G_w = np.matmul(g_output_w, G_output) \n",
    "    G_b = np.sum(G_output, axis=0)\n",
    "    \n",
    "    weight -= LEARNING_RATE * G_w\n",
    "    bias -= LEARNING_RATE * G_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "material-bulgaria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/likelion/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in square\n",
      "  \"\"\"\n",
      "/opt/anaconda3/envs/likelion/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/envs/likelion/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in matmul\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 200 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 300 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 400 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 500 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 600 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 700 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 800 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 900 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "Epoch 1000 : Train - loss =   nan. accuracy =   nan / Test =   nan\n",
      "\n",
      " 최종 테스트 결과 : final accuracy = {:5.3f}.format(final_acc)\n"
     ]
    }
   ],
   "source": [
    "main_exec(epoch_count=1000, mb_size=100, report=100, train_rate=0.7)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "minor-triangle",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-temperature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
