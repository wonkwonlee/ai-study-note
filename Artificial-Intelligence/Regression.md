# 회귀 분석 Regression Analysis

## 신경망의 세가지 기본 출력 유형
![three](https://user-images.githubusercontent.com/28593767/112080349-304b6e00-8bc5-11eb-98d4-ae2dbc1929de.png)

1. 회귀 분석 : 어떤 특징값 하나를 숫자로 추정하여 출력
2. 이진 판단 : '예'나 '아니오' 가운데 한 쪽을 택해 출력
3. 선택 분류 : 몇 가지 후보 중 하나를 골라 선택 결과를 출력


## 회귀 Regression
* 회귀는 옛날 상태로 돌아간다는 의미이다.
* 통계학에서는 연속형 변수 사이의 모형을 구한 뒤 적합성을 측정하는 분석 방법을 말한다.
    + 다시 말해, 입력값들을 근거로 미지의 변숫값을 추정하고 예측하는데 이용된다.
* 딥러닝 알고리즘에서의 값 추정 또한 신경망 모델이 입력 데이터를 근거로 출력값을 추정하는 것이다.

![problem](https://user-images.githubusercontent.com/28593767/112080345-2f1a4100-8bc5-11eb-95c2-cbf5089d817b.png)

* 머신러닝을 적용하기 위해서는 *y = Θ_0 + Θ_1x* 형태의 함수를 만들어야 한다.
    + 세타(Θ)는 구해야 할 미지수, 기울기와 편향을 나타낸다.
* 이러한 매개변수 값을 적절한 값으로 설정하기 위해 머신러닝이 활용된다.
    + 학습 데이터를 기존에 작성에 작성한 함수에 대입하여 세타를 정한다.


## 최소 제곱법 Least Squares Method
![lsm_d](https://user-images.githubusercontent.com/28593767/112080347-2fb2d780-8bc5-11eb-9ca3-e02280b33464.png)

* *y* 와 *fΘ(x)* 의 오차가 0이 되는 것이 가장 이상적이지만 그래프에 있는 모든 점에 생기는 오차를 0으로 만드는 것은 불가능하다.
* 따라서 **모든 점에서 생기는 오차의 합계가 가능한 작아지도록** 한다.

![lsm](https://user-images.githubusercontent.com/28593767/112080351-30e40480-8bc5-11eb-99d5-f24b0872e0cf.png)

* 목적함수 *E(Θ)* 로서, E는 Error의 머릿글자이고 x^(i), y^(i)는 i번째 학습 데이터를 의미한다.
* 각각의 학습 데이터마다 생기는 오차를 제곱하여 모두 더하고 그 값에 1/2을 곱한다.
    + 양수와 음수가 섞여 있는 경우 계산이 어려워지기에 오차가 양수가 되도록 제곱을 수행한다.
    + 또한 제곱의 효과는 오차가 큰 값에 대해서 더 큰 오차값을 부여한다.
    + 전체에 1/2을 곱하는 이유는 결과로 나온 식을 간단한 모양으로 만들기 위해 붙인 상수이다.
    + 최적화 문제에 상수를 붙여도 최솟값이 존재하는 위치는 변하지 않는다.
    + ![square](https://user-images.githubusercontent.com/28593767/112080354-30e40480-8bc5-11eb-831a-4079d2d9e3e9.png)
* 이렇게 **계산된 *E(Θ)* 값이 가장 작아지게 되는 Θ값을 찾는 것이 목적**이고 이것을 **최적화 문제**라고 한다.
* 목적 함수 *E(Θ)* 이 작아진다는 의미는 오차가 작아진다는 의미이고 이러한 기법을 **최소 제곱법**이라고 한다.


## 경사 하강법 Gradient Descent
* 계산한 목적 함수 *E(Θ)* 를 감소시켜야 하기 위해서는 매개변수 *Θ*를 적절하게 조절해야 한다. 
* 하지만 기준값을 비교해 가며 계산하는 방식은 효율적이지 못하다.
* 따라서 *변화하는 정도를 구하기 위해 사용되는 미분*을 활용한다.

![gd](https://user-images.githubusercontent.com/28593767/112080356-317c9b00-8bc5-11eb-831b-4e22672f5ccf.png)

* **𝜂(에타)** 는 **학습률(Learning rate)** 이라 하는 양의 정수로 이 *학습률의 크기에 따라 최솟값에 도달하기까지 갱신해야 하는 횟수가 달라지게 됩니다.* 
* 즉 학습률로 인해 **최솟값에 수렴되는 속도가 달라진다**는 겁니다.
* 학습률이 *적절한 값을 가지면 조금씩 최솟값을 향하여 x값이 줄어들게 된다.*
* 반면 학습률이 너무 크면 x값이 줄어들지 않게 되거나 혹은 최솟값에서 멀어지게 되는데 이것을 **발산**이라 한다.
* 또한 학습률이 너무 작으면 최솟값으로 이동하는 보폭이 작아져 갱신횟수가 늘어나게 된다.

![gd_d](https://user-images.githubusercontent.com/28593767/112080342-2cb7e700-8bc5-11eb-89a3-179d0225a638.png)








