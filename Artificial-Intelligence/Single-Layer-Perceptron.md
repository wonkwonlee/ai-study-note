# 단층 퍼셉트론 Single-Layer Perceptron (SLP)

## 단층 퍼셉트론 신경망 구조
![perc](https://user-images.githubusercontent.com/28593767/111940487-33d1ed00-8b12-11eb-9923-7ec94c9ef3e3.png)

* 단층 퍼셉트론은 **일련의 퍼셉트론을 한 줄로 배치하여 입력 벡터 하나로부터 출력 벡터 하나를 얻어내는 가장 기본적인 신경망 구조**이다.
* 입력 벡터로부터 출력 벡터를 구하려면 출력 벡터의 크기, 즉 **출력 벡터가 담고 있어야 할 스칼라 성분의 수 만큼의 퍼셉트론이 필요**하다.
* *P1, P2, P3*은 크기 3의 출력 벡터를 만들어 내기 위한 퍼셉트론들로 이들 사이에는 어떤 연결도 없어서 서로 영향을 주고 받을 수 없다.
* 각 퍼셉트론은 저마다의 **가중치 벡터와 편향값을 이용하여 입력 벡터 *x*로부터 출력 벡터 *y*를 도출**한다.

![p1](https://user-images.githubusercontent.com/28593767/111940494-36344700-8b12-11eb-919a-2db5ddb0d616.png)

* 첫번째 퍼셉트론 *P1*은 가중치 벡터와 편향값을 이용해 입력 벡터 *x1, x2, x3, x4*로부터 *y1*을 계산한다.
* 즉 단층 퍼셉트론에서 퍼셉트론들은 *입력 벡터만 공유할 뿐 각자의 가중치 벡터와 편향값에 따라 각자의 방식으로 독립적인 정보를 따로따로 생산*한다.
* 퍼셉트론의 가중치 벡터들을 모은 것이 가중치 행렬 *W*이고 편향값들을 모은 것이 편향 벡터 *b*이다.


### 파라미터 Parameter
* 학습 과정 중에 끊임없이 변경되어 가면서 퍼셉트론의 동작 특성을 결정하는 값들을 **파라미터(Parameter)** 혹은 **모델 파라미터(Model parameter)** 라고 한다.
    + 가중치(Weight) 와 편향(Bias)
* 즉, **딥러닝의 학습 목표는 문제 풀이에 적합한 파라미터값의 조합을 구하는 것**이다.
* 딥러닝에서는 퍼셉트론 열을 계층(Layer) 라고 하는데 최종적으로 출력을 생성하는 계층을 **출력 계층(Output layer)**, 출력 계층 앞에서 입력을 처리하여 그 결과를 출력 계층에 전달하는 계층을 **은닉 계층(Hidden layer)** 이라고 한다.
* 단층 퍼셉트론은 은닉 계층 없이 출력 계층 하나만으로 구성되는 구조이다.


## 텐서 연산과 미니 배치의 활용 

### 텐서 Tensor
* 딥러닝에서 텐서란 **다차원 숫자 배열**을 의미한다.
    + 0차원 스칼라, 1차원 벡터, 2차원 행렬, 3차원 이상의 숫자 배열 등
* 텐서가 중요한 이유는 같은 문제라도 반복문 대신 *텐서를 이용해 처리하는 편이 프로그램도 간단하고 처리 속도도 훨씬 빠르기 때문*이다.
    + 파이썬 인터프리터는 반복문보다 텐서 연산을 더 효율적으로 처리하며 특히 병렬 수치 연산을 지원하는 GPU 환경에서 속도 차이는 더욱 커진다.
* 딥러닝에서는 신경망이 여러 데이터를 한꺼번에 처리하는데 이를 **미니 배치(Mini batch)** 라고 한다.

![batch](https://user-images.githubusercontent.com/28593767/111943496-e5741c80-8b18-11eb-813f-421e0a048341.png)

* 위 그림에서 A. 퍼셉트론 하나의 동작 방식, B. 입력 데이터 하나에 대한 단층 퍼셉트론의 동작 방식, C. 입력이 여럿인 미니 배치에 대한 단층 퍼셉트론의 동작 방식을 나타낸다.
* A에서 퍼셉트론은 ***xw** + b* 로 나타내고 벡터의 내적 연산을 이용해 계산한다.
    + 입력 성분의 일차식으로 표현되는 이런 계산 과정을 선형 연산이라고 한다.
* B는 퍼셉트론들이 여럿이니 가중치는 가중치 행렬 ***W***가 되고 편향 역시 벡터 ***b***가 되고 이를 표현하면 ***y = xW + b***가 된다.
    + 이 또한 벡터와 행렬의 연산 형태로 전체 퍼셉트론을 한꺼번에 처리하는 편이 훨씬 효율적이다.
* C는 미니 배치에서의 동작으로 데이터가 여러 개 모여서 입력 행렬 ***X***와 출력 행렬 ***Y***가 되고 이를 표현하면 ***Y = XW + b***가 된다.
    + 마찬가지로 행렬 연산으로 단번에 계산하는 편이 간단하고 효율적이다.
* 딥러닝 프로그램의 효율을 높이려면 최대한 반복문 사용을 피하고 텐서 연산을 이용해 처리하는 것이 중요하다.

### 하이퍼 파라미터 Hyper Parameter
![mini](https://user-images.githubusercontent.com/28593767/111943498-e60cb300-8b18-11eb-8a7e-c558e85c358d.png)

* 미니 배치는 배치 작업보다 *상대적으로 작은 단위로 처리하는 일괄 처리*를 의미한다.
* 미니 배치는 데이터 처리의 효율을 높여주며 개별 학습 데이터의 특징을 무시하지 않으면서도 특징에 너무 휘둘리지 않게 해준다는 점에서 유용하다.
* 학습 데이터 전체에 대한 한 차례 처리를 **에폭(Epoch)** 이라고 한다. 
* 딥러닝에서는 에폭 수나 미니배치 크기처럼 변경되지 않으면서 신경망 구조나 학습 결과에 영향을 미치는 고려 요인들을 하이퍼 파라미터라고 한다
* 하이퍼 파라미터값은 신경망 설계자가 학습 전에 미리 정해주어야 하는 값이며 학습 결과에 큰 영향을 미치는 경우가 많다.
    + 따라서 신경망을 설계할 때는 문제 유형, 신경망 구조, 데이터 양, 학습 결과 등을 종합적으로 살펴보며 하이퍼 파라미터값들을 잘 조절해야 한다.



