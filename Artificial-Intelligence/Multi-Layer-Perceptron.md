# 다층 퍼셉트론 Multi-Layer Perceptron (MLP)

## 다층 퍼셉트론 구조
<img width="1020" alt="mlp" src="https://user-images.githubusercontent.com/28593767/113245140-478b0980-92f1-11eb-83c6-50aa27ee175f.png">

* 다층 퍼셉트론 신경망은 **입력 벡터를 시작으로 하여 중간 표현을 거쳐 출력 벡터를 얻어내는 신경망 구조**로서 다수의 퍼셉트론 계층을 순서를 두고 배치하게 된다.
* *새롭게 추가된 은닉 계층들은 입력 벡터로 부터 차례로 연산을 진행하고 다음 단계에 넘겨주며 최종적으로 출력 계층까지* 이르게 한다.
    + 이렇게 출력에 직접 드러나지 않는 이들 계층을 **은닉 계층(Hidden Layer)** 이라고 하며 은닉 계층이 생성하는 중간 표현을 **은닉 벡터(Hidden Vector)** 라고 한다.
    + 각 계층 안에 속한 퍼셉트론들은 동일한 입력을 공유하지만 각각의 출력 성분을 만들어 낸다. 
* 각 계층 안의 퍼셉트론 끼리는 서로 어떠한 연결이 없기에 영향을 주고 받을 수 없다. 
* 반대로 **인접한 계층끼리는 앞 계층의 출력이 뒤 계층의 모든 퍼셉트론에 공통 입력으로 제공**되어 인접 계층끼리 **완전 연결 방식(Fully-Connected)** 으로 연결된다.

## 오차 역전파법 Backpropagation
![mlp2](https://user-images.githubusercontent.com/28593767/113245154-4b1e9080-92f1-11eb-9144-eaf880716f03.png)

* 신경망에서 가중치나 편향처럼 여러 변수를 사용하다 보니 미분할 때 너무 많은 계산이 필요하다. 
    + *순전파*는 신경망의 입력값에 가중치를 곱하고 편향값을 더한 후 그 값을 다음 계층으로 전달하는 과정이다.
    + 계산하는 순서가 순전파 방식과 반대된다고 하여 *역전파*라는 이름이 붙여졌다.
* 오차 역전파란 이 과정을 거꾸로 돌려 *신경망의 출력값과 실제 정답 사이의 오차를 먼저 구한 다음에 바로 앞 단계의 계층으로 거슬러 올라가며 가중치를 조정하는 기법*이다.
* 즉, **출력값의 오차를 기반으로 출력층에서 입력층 방향으로 가중치와 바이어스를 거꾸로 갱신해 나가는 방법**이라고 할 수 있다.

<img width="778" alt="mlp_3" src="https://user-images.githubusercontent.com/28593767/113245413-dc8e0280-92f1-11eb-90f4-8f7d1b2ea83c.png">

### 기울기 소멸 문제 Vanishing Gradient Problem
* 출력 계층과 다르게 은닉 계층의 수나 폭은 자유롭게 설정될 수 있다.
* 은닉 계층의 수나 폭이 늘어나면 연산 과정이 늘어난다는 의미고 신경망의 품질을 결정짓는 중요한 요인이 될 수 있다는 뜻이다.
* 하지만 은닉 계층을 무작정 늘린다면 *은닉 계층을 많이 거칠수록 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상*이 발생하는데, 이를 **기울기 소멸 문제(Vanishing Gradient Problem)** 라고 한다.
    + 기울기가 0에 가까워 소멸되어 버리면 네트워크의 학습은 매우 느려질만 아니라, 학습이 다 이루어지지 않은 상태에서 멈출 수도 있다.
    + 이러한 현상을 **지역 최솟값(Local Minimum)** 에 도달한다고 표현하기도 합니다.
* 높은 품질의 신경망을 구축하기 위해선 단순히 계층을 늘리는 것 뿐만 아니라 *충분한 양의 양질의 데이터*가 뒷받침되어야 한다.


## 비선형 활성화 함수 Nonlinear Activation Function
* 단층 퍼셉트론에서는 가중치와 편향을 이용해 계산된 선형 연산 결과를 바로 출력했기에 항상 *입력의 일차 함수 형태로 표현*된다.
* 은닉 계층은 이러한 출력 결과를 한번 더 변형시키는데, 이렇게 *선형 연산 결과 후에 적용되는 장치를 비선형 활성화 함수*라고 한다.
* 비선형 활성화 함수는 일차 함수로 표현이 불가능한 좀 더 복잡한 기능을 수행하여 은닉 계층에서 선형 연산 결과를 변형시켜 퍼셉트론의 최종 출력을 만들어 낸다.
* 다층 퍼셉트론에서는 **계층과 계층 사이, 즉 각 계층의 선형 연산 사이마다 비선형 활성화 함수**가 놓이게 된다.
    + 적당한 비선형 활성화 함수를 도입하면 **선형선의 한계**를 벗어날 수 있다.

> 연구결과에 의하면 비선형 활성화 함수를 갖춘 은닉 계층을 충분한 수의 퍼셉트론으로 구성하고 가중치와 편향값만 잘 설정하면, *단 두 계층의 다층 퍼셉트론 구조만으로 어떤 수학적 함수든 원하는 오차 수준 이내로 근사하게 동작*할 수 있다는 것이 증명되었다.
>
> 이러한 이유들로 인하여 계층의 수는 적지만 전체적으로 오히려 많은 노드를 갖는 신경망보다 계층 수는 많아도 노드 수가 적은 신경망으로 문제를 해결하려는 경향이 나타나며 계층을 깊게 쌓는다는 **딥러닝(Deep Learning)** 이 떠오르게 되었다.

### ReLU (Rectified Linear Unit)
<img width="425" alt="relu" src="https://user-images.githubusercontent.com/28593767/113245156-4bb72700-92f1-11eb-9cd8-84d1a158644d.png">

* 은닉 계층의 비선형 활성화 함수로 가장 널리 이용되는 함수 중 하나로 음력값을 걸러내어 0으로 만드는 간단한 기능을 제공한다.
* 소프트맥스나 시그모이드의 수식은 *e* 지수 연산이 포함되어 계산 과정이 복잡하기에 은닉 계층 출력 처리에 적합하지 않다.
* 다만 ReLU 함수는 *x = 0* 에서 미분이 불가능하다는 단점이 있기 때문에 그 지점에서의 미분값을 강제로 정하고 넘어가야 한다.
    + 그래프가 꺾이는 지점인 *x = 0* 일 때 좌우의 기울기가 다르므로 미분값을 정할 수 없다.
    + 따라서 **np.sign()** 함수를 통해 *x = 0* 일 때의 기울기를 0으로 지정한다.

```python
def relu(x):
    return np.maximum(x, 0)

def relu_derv(x):
    return np.sign(x)
```

> ReLU 함수를 통해 이미 정류된 값은 0 혹은 양수의 값만을 지니기 때문에 ReLU의 미분값은 np.sign을 통해 0 혹은 1로 표현할 수 있다.