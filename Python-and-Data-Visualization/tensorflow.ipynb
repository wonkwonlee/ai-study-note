{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-pillow",
   "metadata": {},
   "source": [
    "# 텐서플로 기초\n",
    "\n",
    "> 인공지능의 겨울(AI Winter) 이라 불리는 시기가 두 차례 있었는데 그 중 첫 번째는 퍼셉트론의 한계를 지적한 책 **퍼셉트론**의 발간이 영향을 미쳤다.\n",
    "> \n",
    "> AND, OR, XOR 연산\n",
    "\n",
    "## 난수 Random Number\n",
    "* 신경망의 초깃값을 지정해주는 것을 **초기화(Initialization)** 이라 한다.\n",
    "* 현재 가장 많이 쓰이는 방법은 **Xavier 초기화(Xavier Initialization)**, **He 초기화(He Initialization)** 인데, 이 방법들은 랜덤하지만 어느 정도 규칙성이 있는 범위 내에서 난수를 지정한다.\n",
    "\n",
    "### 균일 분포 Unifrom distribution\n",
    "* 균일 분포란 최솟값과 최댓값 사이의 모든 수가 나올 확률이 동일한 분포\n",
    "* tf.random.uniform() 메서드 사용\n",
    "* 첫번째 인자는 결괏값의 shape, 두번째 인자는 최솟값, 세번째 인자는 최대값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revised-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 버전 확인\n",
    "import tensorflow as tf \n",
    "print(tf.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustained-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.58848536 0.295534  ]\n",
      " [0.00541425 0.890815  ]\n",
      " [0.71302915 0.16843092]\n",
      " [0.7595056  0.8317784 ]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rand = tf.random.uniform([4,2],0,1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-bradley",
   "metadata": {},
   "source": [
    "### 정규 분포 Normal Distribution\n",
    "* 정규 분포는 가운데가 높고 양극단으로 갈수록 낮아지는 분포\n",
    "* tf.random.normal() 메서드 사용\n",
    "* 두번째 인자는 정규 분포의 평균, 세번째 인자는 정규 분포의 표준 편차\n",
    "    + 평균이 0이고 표준 편차가 1일 때 표준 정규 분포라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ordered-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1.0066879  -0.70320606 -0.26885486 -0.61595273], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# mean = 0, std = 1\n",
    "rand = tf.random.normal([4],0,1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-paradise",
   "metadata": {},
   "source": [
    "## 뉴런\n",
    "* 입력 ➡️ 뉴런 ➡️ 출력\n",
    "* 입력 x ➡️ 가중치 w ➡️ 활성화함수 f ➡️ 출력 y\n",
    "* 뉴런에서 학습할 때 변하는 것은 가중치로 처음에는 초기화를 통해 랜덤한 값을 넣고 학습 과정에서 점차 일정한 값으로 수렴하게 된다.\n",
    "* 학습이 잘 된다는 것은 좋은 가중치를 얻어서 원하는 출력에 점점 가까운 값을 얻는 것을 말한다.\n",
    "* 활성화 함수는 시그모이드, ReLU 등이 쓰인다.\n",
    "    + 신경망 초창기에는 시그모이드가 주로 쓰였으니 은닉층이 다수 사용되며 ReLU가 더 많이 쓰인다.\n",
    "* 딥러닝에서 오류를 역전파(Backpropagate) 할 때 시그모이드 함수가 값이 점점 작아지는 문제로 인해 [ReLU 함수](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)가 대안으로 제시되었다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hired-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6211952105557321\n"
     ]
    }
   ],
   "source": [
    "x = 1    # 입력값\n",
    "y = 0    # 출력값\n",
    "w = tf.random.normal([1],0,1)  # 초기 가중치\n",
    "\n",
    "output = sigmoid(x * w)        # 예측값  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-details",
   "metadata": {},
   "source": [
    "### 경사 하강법 Gradient Descent\n",
    "* 실제 출력값(Output)과 기대 출력(y)의 차이를 에러(Error)라고 한다.\n",
    "* 경사 하강법은 w에 입력과 학습률(a)과 에러를 곱해주는 방법으로 *w = w + x * a * error* 라고 나타낸다.\n",
    "    + 학습률(Learning rate)는 w를 업데이트하는 정도로 큰 값으로 설정하면 학습 속도가 빠르지만 과도한 학습으로 적정 수치를 벗어날 우려가 있고 너무 작은 값은 학습 속도가 너무 느려질 수 있다.\n",
    "* 경사 하강법의 경사는 손실 곡선의 기울기를 의미한다.\n",
    "* 경사 하강법은 손실 곡선을 미분한 다음 그 값을 이용하여 가중치가 손실이 가장 낮아지는 지점에 도달하도록 반복적으로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "original-reason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복 횟수 99 : , 에러값 : -0.11086699299456661, 결과값 : 0.11086699299456661\n",
      "반복 횟수 199 : , 에러값 : -0.05468309685183154, 결과값 : 0.05468309685183154\n",
      "반복 횟수 299 : , 에러값 : -0.03588433292958755, 결과값 : 0.03588433292958755\n",
      "반복 횟수 399 : , 에러값 : -0.026614284678820134, 결과값 : 0.026614284678820134\n",
      "반복 횟수 499 : , 에러값 : -0.02111997140483474, 결과값 : 0.02111997140483474\n",
      "반복 횟수 599 : , 에러값 : -0.01749297036828408, 결과값 : 0.01749297036828408\n",
      "반복 횟수 699 : , 에러값 : -0.014922679994953019, 결과값 : 0.014922679994953019\n",
      "반복 횟수 799 : , 에러값 : -0.01300735249812159, 결과값 : 0.01300735249812159\n",
      "반복 횟수 899 : , 에러값 : -0.01152559058929319, 결과값 : 0.01152559058929319\n",
      "반복 횟수 999 : , 에러값 : -0.010345576912598629, 결과값 : 0.010345576912598629\n"
     ]
    }
   ],
   "source": [
    "# Using gradient descent to calculate output\n",
    "\n",
    "# learning rate = 0.1\n",
    "a = 0.1\n",
    "# epoch = 1000\n",
    "for i in range(1000): \n",
    "    output = sigmoid(x * w) \n",
    "    error = y - output\n",
    "    # Training/Update\n",
    "    w = w + x * a * error\n",
    "\n",
    "    # Only print 99-th output (ex 99, 199, 299, ..., 999)\n",
    "    if i % 100 == 99:\n",
    "        print('반복 횟수 {0} : , 에러값 : {1}, 결과값 : {2}'.format(i, error, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manual-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복 횟수 99 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 199 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 299 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 399 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 499 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 599 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 699 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 799 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 899 : , 에러값 : 0.5, 결과값 : 0.5\n",
      "반복 횟수 999 : , 에러값 : 0.5, 결과값 : 0.5\n"
     ]
    }
   ],
   "source": [
    "# 입력이 0이고 출력이 1\n",
    "\n",
    "x = 0\n",
    "y = 1\n",
    "#w = tf.random.normal([1], 0,1) \n",
    "#w = tf.random.uniform([1], 0,1)\n",
    "\n",
    "a = 0.1\n",
    "for i in range(1000): \n",
    "    output = sigmoid(x * w) \n",
    "    error = y - output\n",
    "    w = w + x * a * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print('반복 횟수 {0} : , 에러값 : {1}, 결과값 : {2}'.format(i,error, output))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-insert",
   "metadata": {},
   "source": [
    "> error값도 결과값도 0.5에서 변하지 않는다. 왜냐하면 입력으로 넣은 수가 0이기 때문에 w가 갱신되지 않는다.\n",
    "\n",
    "\n",
    "### 편향 Bias\n",
    "* 편향은 입력으로는 늘 한쪽으로 치우친 고정된 값(b = 1)을 받아서 입력으로 0을 받았을 때 뉴런이 아무것도 배우지 못하는 상황을 방지한다.\n",
    "* 편향 역시 w처럼 난수로 초기화되서 뉴런에 더해져서 출력을 계산하게 된다.\n",
    "* 편향이 더해진 뉴런의 출력 계산식은 *Y = f(X * w + 1 * b)* 라고 나타낸다.\n",
    "\n",
    "![b](https://user-images.githubusercontent.com/28593767/111951384-f3c93500-8b26-11eb-977f-6d60e3d2d39b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exciting-machine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복 횟수 99 : , 에러값 : 0.09497587048894318, 결과값 : 0.9050241295110568\n",
      "반복 횟수 199 : , 에러값 : 0.05030887107118531, 결과값 : 0.9496911289288147\n",
      "반복 횟수 299 : , 에러값 : 0.03391498004509497, 결과값 : 0.966085019954905\n",
      "반복 횟수 399 : , 에러값 : 0.025506195247583796, 결과값 : 0.9744938047524162\n",
      "반복 횟수 499 : , 에러값 : 0.020412600693768157, 결과값 : 0.9795873993062318\n",
      "반복 횟수 599 : , 에러값 : 0.017003248766216505, 결과값 : 0.9829967512337835\n",
      "반복 횟수 699 : , 에러값 : 0.01456395253830911, 결과값 : 0.9854360474616909\n",
      "반복 횟수 799 : , 에러값 : 0.012733469788436591, 결과값 : 0.9872665302115634\n",
      "반복 횟수 899 : , 에러값 : 0.011309796894496293, 결과값 : 0.9886902031055037\n",
      "반복 횟수 999 : , 에러값 : 0.010171179452362344, 결과값 : 0.9898288205476377\n"
     ]
    }
   ],
   "source": [
    "# 편향 추가\n",
    "\n",
    "x = 0\n",
    "y = 1\n",
    "w = tf.random.normal([1], 0,1) \n",
    "b = tf.random.uniform([1], 0,1)\n",
    "\n",
    "a = 0.1\n",
    "for i in range(1000): \n",
    "    output = sigmoid(x * w + 1 * b) \n",
    "    error = y - output\n",
    "    w = w + x * a * error\n",
    "    b = b + 1 * a * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print('반복 횟수 {0} : , 에러값 : {1}, 결과값 : {2}'.format(i,error, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
